[{"categories":["机器学习"],"content":"这篇文章想写很久了，最近终于解决了在博客中插入Echarts的问题，于是终于把它写完了。这篇文章主要是尝试对L1和L2正则化做了可视化，使用交叉熵损失函数使用基础损失函数。由于在其它地方也没有看到过类似的东西，所以觉得写一篇出来还是有点意义的。 3D可视化的图像可以帮助对损失函数，正则化的理解。并且也能直观地解释一些问题，比如为什么L1正则化会导致稀疏模型，会产生特征选择的效果。 正则化的概念了解很久了，但是第一次觉得正则化中的某些东西会成为一个问题，还是看到了小熊猫的B站视频里关于尖点的描述(在24:20)。于是我产生了好奇，如果把带正则化项的损失函数可视化出来，会是什么样的呢？于是就有了这篇文章。 ","date":"2021-04-15","objectID":"/posts/2021/cross-entropy-loss-visualized/:0:0","tags":["机器学习","深度学习","可视化","正则化"],"title":"L1 \u0026 L2 正则化是长什么样子的？","uri":"/posts/2021/cross-entropy-loss-visualized/"},{"categories":["机器学习"],"content":"1. 交叉熵损失 ¶考虑一个简单的神经网络: 一个简单的有2个参数的分类网络simple_nn \"\r一个简单的有2个参数的分类网络\r 这个网络的前向计算公式为: $$\\hat{z_1}=\\beta_1x$$ $$\\hat{z_2}=\\beta_2x$$ $$Softmax(\\hat{z_i}),\\ i\\in{2}$$ 这个网络只有两个参数$\\beta_1$和$\\beta_2$， 考虑使用交叉熵损失函数: $$J(\\beta)=-p\\log(q)-(1-p)\\log(1-q)$$ $$=-p\\log(\\frac{e^{\\beta_1x}}{e^{\\beta_1x}+e^{\\beta_2x}})-(1-p)\\log(\\frac{e^{\\beta_2x}}{e^{\\beta_1x}+e^{\\beta_2x}})$$ $$=…$$ $$=-p\\log{e^{\\beta_1x}}-(1-p)log{e^{\\beta_2x}}+log(e^{\\beta_1x}+e^{\\beta_2x})$$ $$=-p\\beta_1x-(1-p)\\beta_2x+log(e^{\\beta_1x}+e^{\\beta_2x})$$ 这里，$p$代表真实值，$z_1$ 和 $1-p$ 代表 $z_2$ 的真实标签, $\\beta_1$ 和 $\\beta_2$ 是模型参数, $x$ 代表模型输入，是一个标量（只有一个x）. 然后，根据此公式，交叉熵损失函数就可以被可视化为： （以下所有3D图均可拖动、放大、缩小） \r\r\rvar myChart = echarts.init(document.getElementById('echarts400'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0;\\r\\n l2a = 0;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r (为了方便，p和x都被设定为了1，因为我们目前只关心损失是如何根据模型参数$\\beta_1$和$\\beta_2$变化的) 在上面的图中我们可以看到一个平滑的曲面，这个曲面在$\\beta_1$趋近于正无穷，$\\beta_2$趋近于负无穷时取得最小值。可以大致理解为，如果没有正则化项，那么梯度下降法会尽可能让$\\beta_1$和$\\beta_2$变得非常大(小)，以此可能会更容易得到一个过拟合的模型。 ","date":"2021-04-15","objectID":"/posts/2021/cross-entropy-loss-visualized/:1:0","tags":["机器学习","深度学习","可视化","正则化"],"title":"L1 \u0026 L2 正则化是长什么样子的？","uri":"/posts/2021/cross-entropy-loss-visualized/"},{"categories":["机器学习"],"content":"2. 带L1正则化的交叉熵损失 ¶为了避免参数值过大（过小），也就是避免过拟合，我们可以使用正则化。 L1正则化是在损失函数上加入参数的一阶范数之和： $$J(\\beta)=-p\\beta_1x-(1-p)\\beta_2x+log(e^{\\beta_1x}+e^{\\beta_2x})+\\lambda{(||\\beta_1||_1+||\\beta_2||_1)}$$ 其中$\\lambda$为L1正则化系数。 下面的图展示了在不同L1正则化系数的情况下，对损失函数图像的影响： \r\r\rvar myChart = echarts.init(document.getElementById('echarts401'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L1 Reg\\\\n(λ=0.2)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0.2;\\r\\n l2a = 0;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r \r\r\rvar myChart = echarts.init(document.getElementById('echarts402'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L1 Reg\\\\n(λ=0.4)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0.4;\\r\\n l2a = 0;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r \r\r\rvar myChart = echarts.init(document.getElementById('echarts403'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L1 Reg\\\\n(λ=0.6)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\","date":"2021-04-15","objectID":"/posts/2021/cross-entropy-loss-visualized/:2:0","tags":["机器学习","深度学习","可视化","正则化"],"title":"L1 \u0026 L2 正则化是长什么样子的？","uri":"/posts/2021/cross-entropy-loss-visualized/"},{"categories":["机器学习"],"content":"3. 带L2正则化的交叉熵损失 ¶下面是带L2正则化的交叉熵损失公式： $$J(\\beta)=-p\\beta_1x-(1-p)\\beta_2x+log(e^{\\beta_1x}+e^{\\beta_2x})+\\Omega{(||\\beta_1||_2^2+||\\beta_2||_2^2)}$$ 下面的一系列图展示了在不同L2正则化系数Ω的情况下，损失函数的样子： \r\r\rvar myChart = echarts.init(document.getElementById('echarts399'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L2 Reg\\\\n(Ω=0.1)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0;\\r\\n l2a = 0.1;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r \r\r\rvar myChart = echarts.init(document.getElementById('echarts398'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L2 Reg\\\\n(Ω=0.2)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0;\\r\\n l2a = 0.2;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r \r\r\rvar myChart = echarts.init(document.getElementById('echarts397'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L2 Reg\\\\n(Ω=0.3)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"","date":"2021-04-15","objectID":"/posts/2021/cross-entropy-loss-visualized/:3:0","tags":["机器学习","深度学习","可视化","正则化"],"title":"L1 \u0026 L2 正则化是长什么样子的？","uri":"/posts/2021/cross-entropy-loss-visualized/"},{"categories":["机器学习"],"content":"4. 同时带有L1和L2正则化的交叉熵损失 ¶L1和L2正则化可以同时生效，如下图: \r\r\rvar myChart = echarts.init(document.getElementById('echarts395'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L1+L2 Reg\\\\n(λ=0.1，Ω=0.4)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"β2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0.4;\\r\\n l2a = 0.1;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r ","date":"2021-04-15","objectID":"/posts/2021/cross-entropy-loss-visualized/:4:0","tags":["机器学习","深度学习","可视化","正则化"],"title":"L1 \u0026 L2 正则化是长什么样子的？","uri":"/posts/2021/cross-entropy-loss-visualized/"},{"categories":["机器学习"],"content":"5. 结论 ¶L1正则化 对参数的绝对值之和进行惩罚，导致稀疏模型 稀疏模型会有参数选择的效果 稀疏模型更简单且更容易解释，但是比较不容易学习到复杂的关系（毕竟很多不重要的参数值可能都变为了0） 对样本异常点比较稳定，不太容易受其干扰 L2正则化 对参数的平方项进行惩罚，可以得到稠密模型 稠密模型一般来讲具有更好的模型精确度 对样本异常点比较敏感 更多阅读： 机器学习中的正则化是什么意思？ - 管他叫大靖的文章 - 知乎 https://zhuanlan.zhihu.com/p/62615141 ","date":"2021-04-15","objectID":"/posts/2021/cross-entropy-loss-visualized/:5:0","tags":["机器学习","深度学习","可视化","正则化"],"title":"L1 \u0026 L2 正则化是长什么样子的？","uri":"/posts/2021/cross-entropy-loss-visualized/"},{"categories":["生活点滴"],"content":"本文讲述如何从联想官方驱动中提取在Linux/树莓派上安装打印机所需的PPD文件，以供CUPS打印系统使用。","date":"2020-07-25","objectID":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/","tags":["树莓派","打印机"],"title":"用树莓派把联想M7605D打印机变成网络打印机","uri":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["生活点滴"],"content":"最近把家里的有线打印机，通过树莓派变成了网络打印机，可以在家里不同设备上远程打印，十分方便。 关于如何用树莓派连接打印机，网上资料一搜一大堆，例如：如何正确地用树莓派共享打印机，使用树莓派搭建无线打印机，How to add a printer to your raspberry pi or other Linux Computer，所以这不是这篇文章主要想讲的内容。 由于在Linux上使用CUPS安装打印机时，需要用到打印机的PPD驱动文件，而CUPS自带的驱动列表里完全没有联想的打印机，所以这篇文章主要想讲一讲如何从联想M7605D的官方驱动中提取所需的PPD文件，其它型号的打印机或许也可以使用这个方法。 ","date":"2020-07-25","objectID":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/:0:0","tags":["树莓派","打印机"],"title":"用树莓派把联想M7605D打印机变成网络打印机","uri":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["生活点滴"],"content":"PPD文件 ¶PostScript Printer Description (PPD) 文件是由Adobe公司开发的一种用来描述打印机所有支持的功能和特性的文件，它可以让系统知道如何处理和操作打印机。(Wikipedia) ","date":"2020-07-25","objectID":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/:1:0","tags":["树莓派","打印机"],"title":"用树莓派把联想M7605D打印机变成网络打印机","uri":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["生活点滴"],"content":"找到驱动包中的PPD文件 ¶从官方驱动下载地址下载驱动后，解压，在/install目录里可以看到很多不同型号的目录。 驱动页面M7605D驱动页面 \"\r驱动页面\r ‘/install’文件夹\"\r‘/install’文件夹\r 在M7605D的目录的一个文件/install/M7605D/chneng/Brinst_Lang.ini中可以找到包含M7605D的PPD驱动位置： M7605D的PostScript驱动位置\"\rM7605D的PostScript驱动位置\r 进入到目录后就可以看到后缀为.pp_的文件，这其实就是压缩后的.ppd文件 M7605D应该是没有直接自己的驱动，而是跟这两个型号的驱动共享\"\rM7605D应该是没有直接自己的驱动，而是跟这两个型号的驱动共享\r ","date":"2020-07-25","objectID":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/:2:0","tags":["树莓派","打印机"],"title":"用树莓派把联想M7605D打印机变成网络打印机","uri":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["生活点滴"],"content":"解压缩 .pp_文件 ¶原先我看到这些乱码以及若隐若现的Adobe, PPD字样，以为这个文件是被联想加密了，后来发现并不是，它只是被一种比较古老的压缩算法压缩了而已 文件头部的 ‘SZDD’ 表明它是一种压缩格式，可以在网上搜到\"\r文件头部的 ‘SZDD’ 表明它是一种压缩格式，可以在网上搜到\r SZDD在Wikipedia上的描述\"\rSZDD在Wikipedia上的描述\r 维基上说这个文件可以用MS-DOS的EXPAND命令解压，试了一下的确可以，后来又发现其实7-ZIP可以直接解压这个格式。 使用7-ZIP进行解压，或者在CMD中使用Expand命令\"\r使用7-ZIP进行解压，或者在CMD中使用Expand命令\r 最终，ppd文件就被成功提取出来了 Done!\"\rDone!\r","date":"2020-07-25","objectID":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/:3:0","tags":["树莓派","打印机"],"title":"用树莓派把联想M7605D打印机变成网络打印机","uri":"/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["生活点滴"],"content":"最近我想到一个新方法，完全消除了旧电脑机箱里机械硬盘发出的噪音。","date":"2020-07-24","objectID":"/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/","tags":["生活","电脑","噪音"],"title":"如何消除旧电脑机箱中机械硬盘发出的噪音？","uri":"/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/"},{"categories":["生活点滴"],"content":"在电脑机箱中，噪音产生的源头除了各种散热器，另一个大头就是机械硬盘了。尤其是对于使用了N年的老电脑来讲，硬盘内部零件磨损导致的转动不均匀，可以让硬盘产生的噪音成倍增加，再加上机箱的共鸣，整个机箱就变成了一个轰隆隆的大机器，十分烦人。 对于这个问题，一般好一些的机箱会使用橡胶钉取代金属螺丝钉来固定硬盘，达到缓震消除共鸣的目的。然而老机箱并没有这样的条件，怎么办呢？ ","date":"2020-07-24","objectID":"/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/:0:0","tags":["生活","电脑","噪音"],"title":"如何消除旧电脑机箱中机械硬盘发出的噪音？","uri":"/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/"},{"categories":["生活点滴"],"content":"缓震 ¶在想到新的办法之前，我首先想到的是这个办法，思路也是缓震。在硬盘的周围贴上橡胶条，然后放弃使用原先的硬盘槽位，转而直接将硬盘放在机箱之中。 旧方法，在硬盘周围裹上一圈橡胶条，然后直接放在机箱中\"\r旧方法，在硬盘周围裹上一圈橡胶条，然后直接放在机箱中\r 这么做以后效果立竿见影，比起原先机箱发出的巨大轰鸣声，现在基本上只有仔细听才能听到。并且如果开了空调，那空调的声音已经完全能够把它盖掉了。 ","date":"2020-07-24","objectID":"/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/:1:0","tags":["生活","电脑","噪音"],"title":"如何消除旧电脑机箱中机械硬盘发出的噪音？","uri":"/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/"},{"categories":["生活点滴"],"content":"一个更好的方法 ¶上面的那个方法，虽然已经完全够用，但偶尔产生的噪音还是会引起我的注意。 于是我想到了一个新点子，如果用橡皮筋把硬盘架空，让硬盘接触不到机箱内壁，岂不是可以完全消除噪音？ 新方法，在机箱空位中绑上一些橡皮筋，然后把硬盘直接架在上面\"\r新方法，在机箱空位中绑上一些橡皮筋，然后把硬盘直接架在上面\r 效果拔群！ 至此，这台8年的老电脑终于不再轰隆隆了！ ","date":"2020-07-24","objectID":"/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/:2:0","tags":["生活","电脑","噪音"],"title":"如何消除旧电脑机箱中机械硬盘发出的噪音？","uri":"/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/"},{"categories":null,"content":"​ 你好呀，茫茫人海之中你能看到我的博客，这是一种多么的幸运！ 我是Simon，是一个技术爱好者，希望我的博客能对你有所帮助。 ","date":"2020-06-28","objectID":"/about/:0:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"我是… ¶ 一个梦想家 非计算机出身，但热爱计算机的技术宅，目前是一名NLPer 爱钻研，一种由希望从更深层次理解事物的心理所驱动的行为，简单来讲，好奇心比较强，不过，偶尔也容易钻牛角尖 不喜欢记名字，比起事物的名称更喜欢理解其作用与联系，喜欢费曼学习法 爱好音乐，游戏，跑步 ","date":"2020-06-28","objectID":"/about/:0:1","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"建博客的目的 ¶ 分享想法 记录生活 展示自己 2020年，互联网信息过载的年代，我选择为信息的爆炸出一份力（开个玩笑） 提醒自己，不忘初心 (2021.04.15) \r2. **多维度情感分析系统：**\r可以在20个维度上对餐厅评价进行信息提取，数据集来自AI Challenger 2018。\rDemo地址：Sentiment System [备用链接]\r\r--\r","date":"2020-06-28","objectID":"/about/:0:2","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"联系我 ¶邮箱：yihua.zhou@outlook.com WeChat: astro1boy Github: @SimZhou Twitter: @zhouyh0102 (P.S. Twitter 很少用，微信或邮件可以确保联系到我，加我记得备注来由) ​ ","date":"2020-06-28","objectID":"/about/:0:3","tags":null,"title":"关于我","uri":"/about/"},{"categories":[],"content":"建设中… ","date":"0001-01-01","objectID":"/gallery/:0:0","tags":[],"title":"我的相册","uri":"/gallery/"}]