(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{404:function(t,a,e){"use strict";e.r(a);var _=e(42),r=Object(_.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"第12节-关键词提取-ner-依存分析-nlp相关任务"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#第12节-关键词提取-ner-依存分析-nlp相关任务"}},[t._v("#")]),t._v(" 第12节 关键词提取，NER，依存分析，NLP相关任务")]),t._v(" "),e("p",[t._v("这一节主要学了：关键词提取的方法，包括TF-IDF，Text-Rank，机器学习法（机器学习法讲了LDA隐狄利克雷分布，词向量方法）；实体命名识别(NER)；依存分析、语法树；以及各类NLP任务")]),t._v(" "),e("blockquote",[e("p",[t._v("一开始课上纠正了之前的一个错误：")]),t._v(" "),e("p",[t._v("seq2seq中，由于最终生成句子的概率为每一条的条件概率相乘，因此越长的句子最终概率就会越小，最终导致模型更容易生成短句。")]),t._v(" "),e("p",[t._v("解决方法应该是：先对P(y|x)取log，然后再乘以一个归一化项 "),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7BT%5E%5Calpha%7D",alt:""}}),t._v("（T和长度正相关，长度越长，项越小）。使得越负的值，缩减效应更大，限制其负的程度。")])]),t._v(" "),e("h3",{attrs:{id:"_1-tf-idf"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-tf-idf"}},[t._v("#")]),t._v(" 1. TF-IDF")]),t._v(" "),e("p",[t._v("先计算"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?TermFrequency_%7Bt,d%7D=%5Cbegin%7Bcases%7Dtf_%7Bt,d%7D=count(t,d)%5Ctf_%7Bt,d%7D=log_%7B10%7D%7Bcount(t,d)+1%7D%5Cend%7Bcases%7D",alt:""}})]),t._v(" "),e("p",[t._v("然后根据语料库计算"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?InverseDocumentFrequency(IDF)_%7Bt%7D=log_%7B10%7D%7B%5Cfrac%7BN%7D%7Bn_%7Bt%7D%7D%7D",alt:""}})]),t._v(" "),e("p",[t._v("最后"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?w_%7Bt,d%7D=TF_%7Bt,d%7D%5Ctimes%7BIDF_t%7D",alt:""}})]),t._v(" "),e("p",[e("strong",[t._v("Note:")]),t._v(" TF-IDF实际上也是句子/文章表征的一种形式，是一种词袋模型(Bag-of-Words)，即不考虑词语之间的顺序关系，而是简单粗暴地把它们装进一个袋子里面，作为句子/文章的特征。")]),t._v(" "),e("p",[t._v("TF-IDF的优点是效果其实还不错，缺点是语料库很大的话维度会非常大（语料库中token个数的维度）")]),t._v(" "),e("h3",{attrs:{id:"_2-page-rank-text-rank-graph-based-extraction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-page-rank-text-rank-graph-based-extraction"}},[t._v("#")]),t._v(" 2. Page-Rank & Text-Rank（Graph based extraction）")]),t._v(" "),e("h4",{attrs:{id:"_2-1-page-rank"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-page-rank"}},[t._v("#")]),t._v(" 2.1 Page rank")]),t._v(" "),e("p",[t._v("假设：一个网页如果连接的网页越多，那么这个网页越重要")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://uricc.ga/images/2019/12/31/_20191231214446.png",alt:""}})]),t._v(" "),e("p",[e("strong",[t._v("算法:")]),t._v(" 迭代")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?At;t=0:PR(p_i,0)=%5Cfrac%7B1%7D%7BN%7D",alt:""}}),t._v("。意味着一开始用平均值初始化每个Page(i)的PageRank权重。")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?At;each;time;step:;PR(Pi,t+1)=(1-d)+d*%5Csum_%7Bj%5Cin%7BIn(P_i)%7D%7D%7B%5Cfrac%7B1%7D%7B%7COut(P_j)%7C%7DPR(P_j,t)%7D",alt:""}}),t._v("。在每个新的time step, 某个Page(i)的新的权重等于【所有进入Page(i)的那些其他Pages的1/|出度|乘以它们自己的当前PR值】。")]),t._v(" "),e("h4",{attrs:{id:"_2-2-textrank"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-textrank"}},[t._v("#")]),t._v(" 2.2 TextRank")]),t._v(" "),e("p",[t._v("TextRank是用PageRank的思想做文本特征提取的一种模型，具体方法是对文档取滑动窗口，默认滑动窗口内的词语互相两两连接。")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?WS(V_i)=(1-d)+d*%5Csum_%7Bj%5Cin%7BIn(V_i)%7D%7D%7B%5Cfrac%7Bw_%7Bji%7D%7D%7B%5Csum_%7BV_k%5Cin%7BOut(V_j)%7D%7D%7Bw_%7Bjk%7D%7D%7DWS(V_j)%7D",alt:""}})]),t._v(" "),e("p",[t._v("其中"),e("strong",[t._v("WS")]),t._v("是"),e("strong",[t._v("Weighted Sum")]),t._v("的缩写，代表该词在句子/文章中的重要性。")]),t._v(" "),e("p",[t._v("TextRank与PageRank唯一的不同之处是多了edge之间的权重"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?w",alt:""}}),t._v("。")]),t._v(" "),e("p",[t._v("参考资料："),e("a",{attrs:{href:"https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0",target:"_blank",rel:"noopener noreferrer"}},[t._v("[TowardsDataScience] TextRank for Keyword Extraction"),e("OutboundLink")],1)]),t._v(" "),e("h3",{attrs:{id:"_3-lda主题模型-latent-dirichlet-allocation隐含狄利克雷分布"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-lda主题模型-latent-dirichlet-allocation隐含狄利克雷分布"}},[t._v("#")]),t._v(" 3. LDA主题模型 (Latent Dirichlet Allocation隐含狄利克雷分布)")]),t._v(" "),e("p",[t._v("假设：")]),t._v(" "),e("ol",[e("li",[t._v("一个文本的主题服从某个分布")]),t._v(" "),e("li",[t._v("每个主题下的词语服从另一个分布")])]),t._v(" "),e("h3",{attrs:{id:"_4-依存分析-dependency-parsing"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-依存分析-dependency-parsing"}},[t._v("#")]),t._v(" 4. 依存分析（Dependency Parsing）")]),t._v(" "),e("p",[t._v("指的是语法和语义的依存关系的分析，一张图理解：")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://uricc.ga/images/2020/01/06/_20200106151459.png",alt:""}})]),t._v(" "),e("p",[t._v("(作业中提取对话的方法：对句子做依存分析，然后提取句子里主谓关系中的谓语有代表“说”含义的句子，然后就可以找出 "),e("strong",[t._v("XXX说XXX")]),t._v(" 的句子了)")]),t._v(" "),e("h3",{attrs:{id:"_5-nlp任务-分类和生成"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-nlp任务-分类和生成"}},[t._v("#")]),t._v(" 5. NLP任务：分类和生成")]),t._v(" "),e("ol",[e("li",[e("p",[t._v("分类任务")]),t._v(" "),e("p",[t._v("Text Representation")]),t._v(" "),e("p",[t._v("Machine Learning")]),t._v(" "),e("p",[t._v("Deep Learning")])]),t._v(" "),e("li",[e("p",[t._v("生成任务")]),t._v(" "),e("p",[t._v("机器翻译MT")]),t._v(" "),e("p",[t._v("​\t\tStatistics + EM Algorithm (IBM 1)")]),t._v(" "),e("p",[t._v("​\t\tRNN")]),t._v(" "),e("p",[t._v("​\t\tRNN + Attention")]),t._v(" "),e("p",[t._v("​\t\tTransformer")]),t._v(" "),e("p",[t._v("问题回答QA")]),t._v(" "),e("p",[t._v("...")]),t._v(" "),e("p",[t._v("​")])])])])}),[],!1,null,null,null);a.default=r.exports}}]);