(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{408:function(t,a,_){"use strict";_.r(a);var r=_(42),e=Object(r.a)({},(function(){var t=this,a=t.$createElement,_=t._self._c||a;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h2",{attrs:{id:"第16节-深度学习高阶知识-强化学习"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#第16节-深度学习高阶知识-强化学习"}},[t._v("#")]),t._v(" 第16节 深度学习高阶知识——强化学习")]),t._v(" "),_("p",[t._v("这一节学了：强化学习。")]),t._v(" "),_("p",[t._v("目录：")]),t._v(" "),_("ul",[_("li",[t._v("概念")]),t._v(" "),_("li",[t._v("Bellman等式")]),t._v(" "),_("li",[t._v("动态规划")]),t._v(" "),_("li",[t._v("蒙特卡洛")]),t._v(" "),_("li",[t._v("TD")]),t._v(" "),_("li",[t._v("Q-learning 和 SARSA")]),t._v(" "),_("li",[t._v("Deep Q-Network 和 Policy gradient")])]),t._v(" "),_("h3",{attrs:{id:"_1-强化学习概念"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-强化学习概念"}},[t._v("#")]),t._v(" 1. 强化学习概念")]),t._v(" "),_("p",[t._v("强化学习是一个 Agent 与 Environment 相互交互的过程")]),t._v(" "),_("p",[t._v("​\t   state s∈"),_("strong",[t._v("S")])]),t._v(" "),_("p",[t._v("​      action a∈"),_("strong",[t._v("A")])]),t._v(" "),_("p",[t._v("​     ——————")]),t._v(" "),_("p",[t._v("​    |\t\t\t\t\t ↓")]),t._v(" "),_("p",[t._v("Agent\t\tEnvironment")]),t._v(" "),_("p",[t._v("​\t↑\t\t\t\t\t |")]),t._v(" "),_("p",[t._v("​     ——————")]),t._v(" "),_("p",[t._v("​\t  Get reward "),_("strong",[t._v("r")])]),t._v(" "),_("p",[t._v("​    New state s'∈"),_("strong",[t._v("S")])]),t._v(" "),_("p",[t._v("其中重要的概念有如下：")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("States状态空间：包含所有可能的状态")])]),t._v(" "),_("li",[_("p",[t._v("Actions动作空间：包含所有可能的动作")])]),t._v(" "),_("li",[_("p",[t._v("Rewards奖励：R（s, a）在不同的状态采取相同或不同的动作都有对应的一个奖励")])]),t._v(" "),_("li",[_("p",[t._v("Values期望值：总奖励")])]),t._v(" "),_("li",[_("p",[t._v("Policy：策略，代表在某个状态s的最佳动作action")])]),t._v(" "),_("li",[_("p",[t._v("Episode：通过一系列动作经过一系列状态后，获得的一个序列，例如：S1 A1 R2 S2 A2 R3 S3 …… ST（终止）")])])]),t._v(" "),_("p",[_("strong",[t._v("Model Transition（数学描述状态转移）")])]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?P(S%5E%7B'%7D,r%7CS,a)=P%5BS_%7Bt+1%7D=s%5E%7B'%7D,R_%7Bt+1%7D=r%7CS_t=s,A_t=a%5D=%5Csum_%7Br%5Cin%7BR%7D%7DP(s%5E%7B'%7D,r%7CS,a)",alt:""}})]),t._v(" "),_("p",[t._v("期望Reward: "),_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?R(s,a)=E%5BR_%7Bt+1%7D%7CS_t,A_t=a%5D",alt:""}})]),t._v(" "),_("p",[_("strong",[t._v("Value Function and action-value(Q-value) （数学描述奖励函数）")])]),t._v(" "),_("p",[t._v("Rewards: "),_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?G_t=R_%7Bt+1%7D+%5Cgamma%7BR_%7Bt+2%7D%7D+%5Cgamma%5E%7B2%7D%7BR_%7Bt+3%7D%7D+%5Cgamma%5E%7B3%7D%7BR_%7Bt+4%7D%7D+%5Ccdots",alt:""}})]),t._v(" "),_("p",[t._v("State values: "),_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?V_%5Cpi(s)=E_%5Cpi%5BG_t%7CS_t=s%5D",alt:""}}),t._v("，是在某个状态s下，Rewards的期望值。适用于路线不唯一且具有随机性的步骤。")]),t._v(" "),_("p",[t._v("Q-values："),_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?Q_%5Cpi(s,a)=E_%5Cpi%5BG_t%7CS_t=s,A_t=a%5D",alt:""}}),t._v("，是在某个状态s下，已经采取动作a后，Rewards的期望值。")]),t._v(" "),_("p",[t._v("A-values："),_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?Q_%5Cpi(s,a)-V_%5Cpi(s)",alt:""}}),t._v("，代表了采取步骤后，所获得的奖励与平均值的差值，如果它高于平均值（A-value＞0），那么该动作应该来讲是比较好的选择。")]),t._v(" "),_("h3",{attrs:{id:"_2-bellman等式-奖励函数的计算方法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_2-bellman等式-奖励函数的计算方法"}},[t._v("#")]),t._v(" 2. Bellman等式 - 奖励函数的计算方法")]),t._v(" "),_("p",[t._v("Bellman equation是求解每个状态s的V值的过程，只要求解出每个状态的V值后，我们就可以通过每次都选择最大V值来获得最优策略。")]),t._v(" "),_("p",[_("strong",[t._v("Bellman equation (1):")])]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cbegin%7Baligned%7DV(s)&=E%5BG_t%7CS_t=s%5D%5C&=E%5BR_%7Bt+1%7D+%5Cgamma%7BR_%7Bt+2%7D%7D+%5Cgamma%5E%7B2%7D%7BR_%7Bt+3%7D%7D+%5Ccdots%7CS_t=s%5D%5C&=E%5BR_%7Bt+1%7D+%5Cgamma(%7BR_%7Bt+2%7D%7D+%5Cgamma%7BR_%7Bt+3%7D%7D+%5Ccdots)%7CS_t=s%5D%5C&=E%5BR_%7Bt+1%7D+%5Cgamma%7BG_%7Bt+1%7D%7CS_t=s%7D%5D%5C&=E%5BR_%7Bt+1%7D+%5Cgamma%7BV(S_%7Bt+1%7D)%7D%7CS_t=s%5D%5Cend%7Baligned%7D",alt:""}})]),t._v(" "),_("p",[t._v("可以看到，这是一个递归，所以要求解V(s)，只需要从末端反推就可以。")]),t._v(" "),_("p",[_("strong",[t._v("Bellman equation (2):")])]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cbegin%7Baligned%7DQ(s,a)&=E%5BR_%7Bt+1%7D+%5Cgamma%7BV(S_%7Bt+1%7D)%7D%7CS_t=s,A_t=a%5D%5C&=E%5BR_%7Bt+1%7D+%5Cgamma%7BE_%7Ba=%5Cpi%7D%5B%7BQ(s_%7Bt+1%7D,a)%5D%7D%7D%7CS_t=s,A_t=a%5D%5Cend%7Baligned%7D",alt:""}})]),t._v(" "),_("p",[t._v("通过求解max(Qa_1, Qa_2, Qa_3,...)来获得当前步最佳策略action。")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://uricc.ga/images/2020/02/06/_20200207001807.png",alt:""}})]),t._v(" "),_("h3",{attrs:{id:"_3-dynamic-programming-奖励函数的求解方法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_3-dynamic-programming-奖励函数的求解方法"}},[t._v("#")]),t._v(" 3. Dynamic Programming - 奖励函数的求解方法")]),t._v(" "),_("p",[_("strong",[t._v("Value iteration 值迭代")])]),t._v(" "),_("p",[t._v("具体做法是：先对所有状态初始化一个值，然后使用Bellman equation"),_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?V(S_t)=r+%5Csum_%7BP%7D%7BP(S_%7Bt+1%7D%7CS_t)%7DV(S_%7Bt+1%7D)",alt:""}}),t._v("进行值的更新，然后不断迭代，直到收敛 |W_t+1 - W_t| ≤ δ：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://uricc.ga/images/2020/02/07/Bellman-iteration.png",alt:""}})]),t._v(" "),_("p",[t._v("其中有两种做法，一种是一次计算，同时更新，另一种是将更新后的值作为下一个格子的输入值进行更新。这两种做法都是可取的。")]),t._v(" "),_("p",[_("strong",[t._v("Policy iteration 策略迭代")])]),t._v(" "),_("p",[t._v("具体做法是：先对所有状态初始化一个随机策略，然后使用第二种Bellman equation，进行最佳策略的更新，然后不断迭代，直到收敛。")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://uricc.ga/images/2020/02/07/Bellman-iteration2.png",alt:""}})]),t._v(" "),_("h3",{attrs:{id:"_4-monte-carlo-method-奖励函数的求解方法2"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_4-monte-carlo-method-奖励函数的求解方法2"}},[t._v("#")]),t._v(" 4. Monte-Carlo Method - 奖励函数的求解方法2")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://uricc.ga/images/2020/02/07/_20200207231312f43e8b578cd59838.png",alt:""}})]),t._v(" "),_("p",[t._v("正常情况下一般来讲，各状态之间的Transition Probability是未知的。")]),t._v(" "),_("p",[t._v("蒙特卡洛是用随机模拟的方式，来估算每一个状态的V值的方法。")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?S_0%5Cxrightarrow%5Br%5D%7Ba_0%7DS_1%5Cxrightarrow%5Br%5D%7Ba_1%7DS_2%5Cxrightarrow%5Br%5D%7Ba_2%7D%5Ccdots%5Cxrightarrow%5Br%5D%7Ba_%7Bt-1%7D%7DS_T;%5Ctextcircled%7B1%7D",alt:""}})]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?S_0%5E%7B'%7D%5Cxrightarrow%5Br%5D%7Ba_0%7DS_1%5E%7B'%7D%5Cxrightarrow%5Br%5D%7Ba_1%7DS_2%5E%7B'%7D%5Cxrightarrow%5Br%5D%7Ba_2%7D%5Ccdots%5Cxrightarrow%5Br%5D%7Ba_%7Bt-1%7D%7DS_T%5E%7B'%7D;%5Ctextcircled%7B2%7D",alt:""}})]),t._v(" "),_("p",[t._v("……")]),t._v(" "),_("p",[t._v("通过不同的模拟，我们可以得到不同的路线，而每次模拟都可以算出路线上每个状态点的"),_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?G_t",alt:""}}),t._v("值。")]),t._v(" "),_("p",[t._v("而真"),_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?G_t",alt:""}}),t._v("值的计算有2种方法：")]),t._v(" "),_("p",[_("strong",[t._v("① First Visited - 每个状态在每条路径只计算第一次")])]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?V(S_1)=%5Cfrac%7BG_t%5E1+G_t%5E2+G_t%5E3+...+G_t%5En%7D%7Bn%7D",alt:""}})]),t._v(" "),_("p",[_("strong",[t._v("② Every Visited - 每个状态碰见几次就是几次")])]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?V(S_1)=%5Cfrac%7BG_t%5E1%7B(S_1)%7D+G_t%5E2%7B(S_1)%7D+G_t%5E3%7B(S_1)%7D+...+G_t%5En%7B(S_1)%7D%7D%7Bn%7D",alt:""}})]),t._v(" "),_("p",[t._v("总体来讲，蒙特卡洛方法bias低（为0），variance高（由于随机采样）")]),t._v(" "),_("p",[t._v("DP法bias高（需要估计概率值），variance低（不需随机采样，直接计算）")]),t._v(" "),_("h3",{attrs:{id:"_5-temporal-difference-learning-奖励函数的求解方法3"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_5-temporal-difference-learning-奖励函数的求解方法3"}},[t._v("#")]),t._v(" 5. Temporal-Difference Learning - 奖励函数的求解方法3")]),t._v(" "),_("ul",[_("li",[t._v("Temporal-Difference Learning是DP与Monte-Carlo的结合。")])]),t._v(" "),_("p",[t._v("即，一部分用蒙特卡洛采样解，一部分用Bellman Equation解。")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Ctextcircled%7B,%7D%5Clongrightarrow%5Ctextcircled%7B,%7D%5Clongrightarrow%5Ctextcircled%7B,%7D%5Clongrightarrow%7BV(S_%7Bt+2%7D)%7D",alt:""}})]),t._v(" "),_("p",[t._v("例如，前面几步用蒙特卡洛，后面几步用DP。（圆圈表示采样）")]),t._v(" "),_("p",[t._v("TD相较于蒙特卡洛，bias升高（需要用概率估计值），variance降低（减少了随机采样）")]),t._v(" "),_("p",[t._v("TD相较于蒙特卡洛，计算更快；相较于DP，不需要知道Transition Probability。")]),t._v(" "),_("h3",{attrs:{id:"_6-q-learning-and-sarsa"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_6-q-learning-and-sarsa"}},[t._v("#")]),t._v(" 6. Q-learning and SARSA")]),t._v(" "),_("p",[t._v("Q-learning: 计算出不同的Q后，Value取值按照max()取")]),t._v(" "),_("p",[t._v("SARSA (State-Action-Reward-State-Action): 计算出不同的Q后，Value取值按照policy取")]),t._v(" "),_("h3",{attrs:{id:"_7-function-approximation-deep-q-network"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_7-function-approximation-deep-q-network"}},[t._v("#")]),t._v(" 7. Function Approximation (Deep Q-Network)")]),t._v(" "),_("p",[t._v("Deep Q-Network的思路是使用神经网络来拟合Q函数：f(S1) → Q值")]),t._v(" "),_("p",[t._v("该神经网络的输入就是State，输出就是Q-Value")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://uricc.ga/images/2020/02/07/Annotation-2020-02-08-020129.png",alt:""}})]),t._v(" "),_("p",[t._v("（例如，Atari游戏中，输入的就是图片像素，使用CNN提取特征）")]),t._v(" "),_("h3",{attrs:{id:"_8-policy-gradient"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_8-policy-gradient"}},[t._v("#")]),t._v(" 8. Policy Gradient")]),t._v(" "),_("p",[t._v("给一个函数，这个函数可以拟合出最佳move，即π的学习。")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?maximize;J(%5Ctheta)=%5Csum_%7Ba%5Cin%7BA%7D%7D%7B%5Cpi(a%7Cs,%5Ctheta)%7DQ_%5Cpi(s,a)",alt:""}})]),t._v(" "),_("p",[_("strong",[t._v("AlphaGo")])]),t._v(" "),_("p",[t._v("先通过人类棋谱学习，再通过自己对弈学习。")]),t._v(" "),_("p",[_("strong",[t._v("AlphaZero")])]),t._v(" "),_("p",[t._v("棋盘输入13层的CNN提取状态特征，并且加上一些额外信息（例如棋盘中可被吃掉的目，等）。")]),t._v(" "),_("p",[t._v("通过自己和自己下棋，更新policy gradient中的参数。其中使用了蒙特卡洛树搜索，剪枝等方法。")])])}),[],!1,null,null,null);a.default=e.exports}}]);