(window.webpackJsonp=window.webpackJsonp||[]).push([[31],{390:function(t,a,e){"use strict";e.r(a);var s=e(42),_=Object(s.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"第10节-机器学习算法（贝叶斯，k-means，svm，随机森林，xgboost-等）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#第10节-机器学习算法（贝叶斯，k-means，svm，随机森林，xgboost-等）"}},[t._v("#")]),t._v(" 第10节 机器学习算法（贝叶斯，k-means，SVM，随机森林，XGBoost 等）")]),t._v(" "),e("p",[t._v("这一节主要学习了：经典机器学习中的贝叶斯方法，K-means，SVM；集成学习中的随机森林，XGBoost")]),t._v(" "),e("h3",{attrs:{id:"_1-贝叶斯方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-贝叶斯方法"}},[t._v("#")]),t._v(" 1. 贝叶斯方法")]),t._v(" "),e("blockquote",[e("p",[t._v("Recap of probability theory:")]),t._v(" "),e("p",[t._v("Independency Assumption:"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?P(A%5Ccap%7BB%7D)=P(A)%5Ccdot%7BP(B)%7D",alt:""}})]),t._v(" "),e("p",[t._v("Rule of Total Probability: "),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?P(A)=%5Csum_i%7BP(A%7CB_i)%5Ccdot%7BP(B_i)%7D%7D",alt:""}})]),t._v(" "),e("p",[t._v("Bayes' Rule: "),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?P(A%7CB)=%5Cfrac%7BP(A%5Ccap%7BB%7D)%7D%7BP(B)%7D=%5Cfrac%7BP(B%7CA)%5Ccdot%7BP(A)%7D%7D%7BP(B)%7D",alt:""}})])]),t._v(" "),e("p",[t._v("先验概率："),e("em",[t._v("指根据以往经验和分析得到的概率")])]),t._v(" "),e("p",[t._v("后验概率："),e("em",[t._v("事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小")])]),t._v(" "),e("p",[e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?P(A%7CB)=%5Cfrac%7BP(A%5Ccap%7BB%7D)%7D%7BP(B)%7D=%5Cfrac%7BP(B%7CA)%5Ccdot%7BP(A)%7D%7D%7BP(B)%7D",alt:""}})]),t._v(" "),e("h4",{attrs:{id:"_1-1-maximum-a-posterior（极大后验概率）、maximum-likelihood（极大似然估计）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-maximum-a-posterior（极大后验概率）、maximum-likelihood（极大似然估计）"}},[t._v("#")]),t._v(" 1.1 Maximum a Posterior（极大后验概率）、Maximum Likelihood（极大似然估计）")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?h_%7BMAP%7D=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7DP(h%7CD)=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7DP(D%7Ch)P(h)",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?h_%7BML%7D=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7DP(D%7Ch)",alt:""}})]),t._v(" "),e("p",[t._v("where D is short for Data, H is the set of Hypothesises, h is a particular function in the hypothesises")]),t._v(" "),e("p",[t._v("简单解释：")]),t._v(" "),e("ul",[e("li",[t._v("在极大后验概率情况下，Data已经发生，我们需要求一个最佳h，它对数据的拟合情况最好（生成概率最大），则这个h就是最佳h")]),t._v(" "),e("li",[t._v("在极大似然的情况下，我们要求一个h，使得在这个h下，我们取得观测数据D的概率最大，则这个h是最佳h")])]),t._v(" "),e("h4",{attrs:{id:"_1-2-朴素贝叶斯-naive-bayes"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-朴素贝叶斯-naive-bayes"}},[t._v("#")]),t._v(" 1.2 朴素贝叶斯 Naive Bayes")]),t._v(" "),e("p",[t._v("朴素贝叶斯分类器，是一个生成模型，对于每一种分类，它都可以生成一个后验概率")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?h_%7By=y_0,%7BMAP%7D%7D%5C=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7DP(y=y_0%7Ca_1,a_2,a_3,%5Ccdots)%5Cnewline=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7D%5Cfrac%7BP(a_1,a_2,a_3,%5Ccdots%7Cy=y_0)%5Ccdot%7BP(y=y_0)%7D%7D%7BP(a_1,a_2,%5Ccdots)%7D%5Cnewline=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7D%7BP(a_1,a_2,a_3,%5Ccdots%7Cy=y_0)%5Ccdot%7BP%7D(y=y_0)%7D%5Cnewline=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7D%7BP(a_1%7Cy_0)%5Ccdot%7BP(a_2%7Cy_0)%7D%5Ccdot%7BP(a_3%7Cy_0)%7D%5Ccdots%7BP(y=y_0)%7D%7D",alt:""}})]),t._v(" "),e("p",[t._v('(Last step according to "Naive" hypothesis)')]),t._v(" "),e("p",[t._v("然后我们同样可以求出"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?h_%7By=y_1%7D",alt:""}}),t._v(", "),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?h_%7By=y_2%7D",alt:""}}),t._v(", ...。最后y的估计值就是使h得到最大值所对于的y值。")]),t._v(" "),e("p",[t._v("例：文档分类")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://uricc.ga/images/2019/12/13/_20191213164304.png",alt:""}})]),t._v(" "),e("p",[t._v("在用朴素贝叶斯进行文档分类的例子中，由于词库中某些词可能在句子中没有出现，产生条件概率为0的情况，因此对于所有词的初始词频默认为1（而不是0）（Add 1 smoothing）")]),t._v(" "),e("h5",{attrs:{id:""}},[e("a",{staticClass:"header-anchor",attrs:{href:"#"}},[t._v("#")])]),t._v(" "),e("h3",{attrs:{id:"_2-k-means"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-k-means"}},[t._v("#")]),t._v(" 2. K-means")]),t._v(" "),e("blockquote",[e("p",[e("strong",[t._v("Input:")]),t._v(" dataset D, number of clusters K")]),t._v(" "),e("p",[e("strong",[t._v("Output:")]),t._v(" partition of D into k clusters")]),t._v(" "),e("p",[e("strong",[t._v("Algorithm:")])]),t._v(" "),e("p",[t._v("​\t\tChoose k random seeds")]),t._v(" "),e("p",[t._v("​\t\tRepeat until no changes:")]),t._v(" "),e("p",[t._v("​\t\t\t\tAssign each instance to the cluster of its closest seed")]),t._v(" "),e("p",[t._v("​\t\t\t\tRedefine seeds as cluster means")]),t._v(" "),e("p",[t._v("​\t\tReturn k clusters")])]),t._v(" "),e("p",[t._v("如何提升聚类效果：k个点的选取尽可能互相远离")]),t._v(" "),e("h3",{attrs:{id:"_3-svm"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-svm"}},[t._v("#")]),t._v(" 3. SVM")]),t._v(" "),e("blockquote",[e("p",[t._v("前置知识：拉格朗日乘子法，用于解决条件约束优化问题。"),e("br")]),t._v(" "),e("p",[e("strong",[t._v("Equility constraints:")])]),t._v(" "),e("p",[t._v("​\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cmin_%7Bx%5Cin%7BR%7D%7Df(x)%5Cquad%5Ctext%7Bs.t.%7Dh(x)=0",alt:""}})]),t._v(" "),e("p",[t._v("​\t令"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?L=f(x)+%5Clambda%5Ccdot%7Bh(x)%7D",alt:""}}),t._v(",")]),t._v(" "),e("p",[t._v("​\t然后L对 x 和 λ 分别求偏导并令它们都等于 0，即可得到最优解")]),t._v(" "),e("p",[t._v("​\t"),e("strong",[t._v("实际意义为：对x求偏导为零，即等同于切线平行："),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Cpartial%7Bf%7D%7D%7B%5Cpartial%7Bx%7D%7D=-%5Clambda%5Ccdot%5Cfrac%7B%5Cpartial%7Bh%7D%7D%7B%5Cpartial%7Bx%7D%7D",alt:""}})])]),t._v(" "),e("p",[t._v("​    \t\t\t\t\t   "),e("strong",[t._v("对λ求偏导，即等同于满足约束条件："),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?h(x)=0",alt:""}})])]),t._v(" "),e("p",[t._v("​\t详解见3B1B youtube视频："),e("a",{attrs:{href:"https://www.youtube.com/embed/yuqB-d5MjZA",target:"_blank",rel:"noopener noreferrer"}},[t._v("Lagrange multipliers, using tangency to solve constrained optimization"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("strong",[t._v("Inequility constrains:")])]),t._v(" "),e("p",[t._v("​\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cmin_%7Bx%5Cin%7BR%7D%7Df(x)%5Cquad%5Ctext%7Bs.t.%7Dg(x)%5Cleq0",alt:""}})]),t._v(" "),e("p",[t._v("​\t令"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?L=f(x)+%5Clambda%5Ccdot%7Bg(x)%7D",alt:""}})]),t._v(" "),e("p",[t._v("​    若最小值点就在约束范围内，则约束条件相当于没有用，")]),t._v(" "),e("p",[t._v("​    若最小值点在约束范围边界上，则约束条件起作用：")]),t._v(" "),e("p",[t._v("​    \tf与g梯度方向相反："),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?-%5Cnabla%7Bf(x)%7D=u%5Ccdot%5Cnabla%7Bg(x)%7D",alt:""}}),t._v(" （即求偏导）且  "),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?u%5Cge%7B0%7D",alt:""}})]),t._v(" "),e("p",[t._v("​        约束条件："),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?g(x)%5Cle%7B0%7D",alt:""}})]),t._v(" "),e("p",[t._v("​\t\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?u%5Ccdot%7Bg(x)%7D=0",alt:""}}),t._v(" （u=0，则最小值点在约束范围内，约束不成立；g(x)=0 则约束条件成立）")]),t._v(" "),e("p",[e("strong",[t._v("KKT条件：")])]),t._v(" "),e("p",[t._v("​\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cmin_%7Bx%5Cin%7BR%7D%7Df(x)%5Cquad%5Ctext%7Bs.t.%7Df(x)=0,%5Cquad%7Bg(x)%7D%5Cleq0",alt:""}})]),t._v(" "),e("p",[t._v("​\t把等式约束和不等式约束加到一起，则满足以下条件的点就是极值点：")]),t._v(" "),e("p",[t._v("​\t\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cnabla_%7Bx%7DL=0",alt:""}})]),t._v(" "),e("p",[t._v("​\t\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cmu%5Ccdot%7Bg(x)%7D=0",alt:""}})]),t._v(" "),e("p",[t._v("​\t\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?h(x)=0",alt:""}})]),t._v(" "),e("p",[t._v("​\t\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?g(x)%5Cleq%7B0%7D",alt:""}})]),t._v(" "),e("p",[t._v("​\t\t"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cmu%5Cgeq%7B0%7D%7D",alt:""}})]),t._v(" "),e("p",[t._v("(这一部分还需消化理解，详见 "),e("em",[t._v("Convex Optimization")]),t._v(" 第6章：对偶问题，以及博客："),e("a",{attrs:{href:"https://blog.csdn.net/fkyyly/article/details/86488582",target:"_blank",rel:"noopener noreferrer"}},[t._v("link"),e("OutboundLink")],1),t._v(")")])]),t._v(" "),e("p",[t._v("SVM 的目的是最大化间隔，即")]),t._v(" "),e("h4",{attrs:{id:"kernel-trick"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kernel-trick"}},[t._v("#")]),t._v(" Kernel Trick")]),t._v(" "),e("p",[t._v("使用非线性函数将数据打到高维空间，使之在高维空间线性可分：")]),t._v(" "),e("p",[e("img",{attrs:{src:"http://uricc.ga/images/2019/12/20/_20191220115256.png",alt:"Kernal Trick"}})]),t._v(" "),e("h4",{attrs:{id:"svm的优缺点"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#svm的优缺点"}},[t._v("#")]),t._v(" SVM的优缺点")]),t._v(" "),e("p",[t._v("优点：")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("凸优化给予"),e("strong",[t._v("全局最优")])])]),t._v(" "),e("li",[e("p",[t._v("用"),e("strong",[t._v("核技巧")]),t._v("可以处理"),e("strong",[t._v("非线性数据")])])]),t._v(" "),e("li",[e("p",[t._v("维度灾难还好 https://www.quora.com/Does-SVM-suffer-from-the-curse-of-dimensionality-If-so-how-does-SVM-overcome-it")])]),t._v(" "),e("li",[e("p",[t._v("可解释")])])]),t._v(" "),e("p",[t._v("缺点：")]),t._v(" "),e("ul",[e("li",[t._v("计算复杂度高（需要遍历所有样本点）")]),t._v(" "),e("li",[t._v("本质上是二分类模型，对多分类问题效果不会特别好（尽管有技巧让它可以用于多分类例如one-to-many）")])]),t._v(" "),e("h3",{attrs:{id:"_4-集成学习（ensemble-learning）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-集成学习（ensemble-learning）"}},[t._v("#")]),t._v(" 4. 集成学习（Ensemble Learning）")]),t._v(" "),e("blockquote",[e("p",[t._v("集成学习的思想是训练多个模型，然后通过投票的方式获取多数模型赞同的预测，以获得更好的结果。")]),t._v(" "),e("p",[t._v("集成学习的分类有：")]),t._v(" "),e("p",[e("strong",[t._v("Bagging：每次通过Bootstrap方式训练一个分类器，各分类器相互独立")])]),t._v(" "),e("p",[e("img",{attrs:{src:"http://uricc.ga/images/2019/12/20/bagging.png",alt:""}})]),t._v(" "),e("p",[e("strong",[t._v("Boosting：每次训练出的模型的学习效果不好的部分，都会提升下一次模型的效果")])]),t._v(" "),e("p",[e("img",{attrs:{src:"http://uricc.ga/images/2019/12/20/boosting.png",alt:""}})])]),t._v(" "),e("h4",{attrs:{id:"_4-1-随机森林"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-随机森林"}},[t._v("#")]),t._v(" 4.1 随机森林")]),t._v(" "),e("p",[t._v("随机森林算法的过程：")]),t._v(" "),e("blockquote",[e("p",[t._v("训练集 S := (x1, y1),...,(xn, yn)，特征空间 F，决策树数目 B.")]),t._v(" "),e("p",[e("strong",[t._v("function RandomForest(S, F)")])]),t._v(" "),e("p",[t._v("​\t\tH ← 0")]),t._v(" "),e("p",[t._v("​\t\t"),e("strong",[t._v("for")]),t._v(" i ∈ 1, ..., B "),e("strong",[t._v("do")])]),t._v(" "),e("p",[t._v("​\t\t\t\tS(i) ← A bootstrap sample from S")]),t._v(" "),e("p",[t._v("​\t\t\t\th(i) ← RandomizedTreeLearn(S(i), F)")]),t._v(" "),e("p",[t._v("​\t\t\t\tH ← H ∪ {h(i)}")]),t._v(" "),e("p",[t._v("​\t\t"),e("strong",[t._v("end for")])]),t._v(" "),e("p",[t._v("​\t\t"),e("strong",[t._v("return H")])]),t._v(" "),e("p",[e("strong",[t._v("end function")])]),t._v(" "),e("p",[e("strong",[t._v("function RandomizedTreeLearn(S, F)")])]),t._v(" "),e("p",[t._v("​\t\tAt each node:")]),t._v(" "),e("p",[t._v("​\t\t\t\tf ← very small subset of F")]),t._v(" "),e("p",[t._v("​\t\t\t\tSplit on best feature in f")]),t._v(" "),e("p",[t._v("​\t\t"),e("strong",[t._v("return The learned tree")])]),t._v(" "),e("p",[e("strong",[t._v("end function")])])]),t._v(" "),e("p",[t._v("随机的体现：")]),t._v(" "),e("ul",[e("li",[t._v("样本随机取得（bootstrap有放回抽样）")]),t._v(" "),e("li",[t._v("特征的随机取得（每棵树只选取一部分特征，比如t个特征）")])]),t._v(" "),e("h4",{attrs:{id:"_4-2-xgboost"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-xgboost"}},[t._v("#")]),t._v(" 4.2 XGBoost")]),t._v(" "),e("blockquote",[e("p",[t._v("预备知识：回归树")]),t._v(" "),e("p",[t._v("​\ttest样本的y值，取的是最后叶子节点中train样本的y值的平均；")]),t._v(" "),e("p",[t._v("​\t选择分类点时，用的不是信息增益，而是最小化类间方差")])]),t._v(" "),e("p",[e("strong",[t._v("目标函数：")]),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?Obj=%5Csum_%7Bi=1%7D%5E%7Bn%7Dl(y_i,%5Chat%7By%7D_i)+%5Csum_%7Bk=1%7D%5E%7BK%7D%5COmega(f_k)",alt:""}})]),t._v(" "),e("p",[t._v("其中，"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?l",alt:""}}),t._v("为loss；"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5COmega",alt:""}}),t._v("为正则化项，表示树的复杂度函数。")]),t._v(" "),e("p",[t._v("其中 "),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5COmega(f)=%5Cgamma%7BT%7D+%5Cfrac%7B1%7D%7B2%7D%5Clambda%5Csum_%7Bj=1%7D%5E%7BT%7Dw_j%5E%7B2%7D",alt:""}}),t._v("，"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?T",alt:""}}),t._v("代表叶子节点的个数，"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?w",alt:""}}),t._v("代表叶子节点的分数，分别由"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Cgamma",alt:""}}),t._v("和"),e("img",{attrs:{src:"http://latex.codecogs.com/gif.latex?%5Clambda",alt:""}}),t._v("控制。")]),t._v(" "),e("p",[t._v("参考："),e("a",{attrs:{href:"https://www.cnblogs.com/zongfa/p/9324684.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("机器学习--boosting家族之XGBoost算法"),e("OutboundLink")],1),t._v("，"),e("a",{attrs:{href:"https://blog.csdn.net/zww275250/article/details/78652522",target:"_blank",rel:"noopener noreferrer"}},[t._v("XGBoost 的前世今生"),e("OutboundLink")],1),t._v("，"),e("a",{attrs:{href:"https://blog.csdn.net/hemeinvyiqiluoben/article/details/87870656",target:"_blank",rel:"noopener noreferrer"}},[t._v("COS 访谈第 18 期：陈天奇"),e("OutboundLink")],1),t._v("，"),e("a",{attrs:{href:"https://xgboost.readthedocs.io/en/latest/tutorials/model.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("XGBoost官方文档"),e("OutboundLink")],1),t._v("，"),e("a",{attrs:{href:"https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("NLP-LOVE的XGBoost介绍"),e("OutboundLink")],1)])])}),[],!1,null,null,null);a.default=_.exports}}]);