(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{352:function(t,_,e){t.exports=e.p+"assets/img/image-20200809200733364.3dde3917.png"},353:function(t,_,e){t.exports=e.p+"assets/img/image-20200809201050891.48372e4f.png"},354:function(t,_,e){t.exports=e.p+"assets/img/image-20200809204653140.18433ca3.png"},374:function(t,_,e){"use strict";e.r(_);var a=e(42),v=Object(a.a)({},(function(){var t=this,_=t.$createElement,a=t._self._c||_;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("本内容按照吴恩达公开课《Machine Learning》的 Lecture Slides 进行分类，每一个H1标题对应一个Lecture Slide，每一个H2标题对应Lecture Slide中的一个小章节。")]),t._v(" "),a("p",[t._v("本内容是课程的简化总结，适合已经了解机器学习基本概念的人进行回顾以及查漏补缺。")]),t._v(" "),a("h1",{attrs:{id:"_6-逻辑回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-逻辑回归"}},[t._v("#")]),t._v(" 6 逻辑回归")]),t._v(" "),a("h2",{attrs:{id:"_6-1-分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-1-分类"}},[t._v("#")]),t._v(" 6.1 分类")]),t._v(" "),a("p",[t._v("逻辑回归用于"),a("strong",[t._v("分类问题")]),t._v("的场景，例如：")]),t._v(" "),a("ul",[a("li",[t._v("邮件是否是垃圾邮件？")]),t._v(" "),a("li",[t._v("订单是否是欺诈订单？")]),t._v(" "),a("li",[t._v("肿瘤是良性还是恶性？")])]),t._v(" "),a("p",[t._v("此时 $y\\in{0,1}$，一般把0当作负例，1当作正例")]),t._v(" "),a("p",[a("strong",[t._v("阈值的选择")]),t._v("：可以自定义，例如：")]),t._v(" "),a("p",[t._v("当 $h_{\\theta}(x)\\geq0.5$ 时，预测 $y=1$")]),t._v(" "),a("p",[t._v("当 $h_{\\theta}(x)\\lt0.5$ 时，预测 $y=0$")]),t._v(" "),a("blockquote",[a("p",[t._v("编者注1：后来我们知道阈值的选择会对应不同的模型表现，通过衡量模型在"),a("strong",[t._v("不同阈值下的总体表现")]),t._v("，就产生了"),a("strong",[t._v("ROC曲线")]),t._v("，通过计算ROC曲线下的面积，就产生了"),a("strong",[t._v("AUC评价指标")]),t._v("。")]),t._v(" "),a("p",[t._v("编者注2：**为什么要使用逻辑回归呢？**通过线性函数拟合，然后选择恰当的阈值来得到结果不可以吗？可以，但是这样做有两个问题：一、阈值的选择会因此变得非常敏感；二、线性函数会因此无法很好地拟合数据。其实本质上来讲，使用sigmoid函数将连续空间转化到[0, 1]空间，就是为了更好地拟合数据分布的空间。")]),t._v(" "),a("p",[t._v("编者注3：进一步思考，如果采用线性模型，仅在损失的计算中应用sigmoid（例如logit loss），那么模型的拟合与逻辑回归其实是一样了，此时也可以通过选择一个恰当的y的阈值来定义模型。此时可以理解为把逻辑回归的sigmod层脱离了出来，这里的y就是逻辑回归中的隐藏层y（加sigmoid函数之前）")])]),t._v(" "),a("h2",{attrs:{id:"_6-2-模型表示"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-2-模型表示"}},[t._v("#")]),t._v(" 6.2 模型表示")]),t._v(" "),a("p",[t._v("在线性回归的基础上：增加一个函数 $g$，使得：")]),t._v(" "),a("p",[t._v("$h_{\\theta}(x)=g(\\theta^Tx)$")]),t._v(" "),a("p",[t._v("如果 $g$ 使用Sigmoid函数(也叫Logistic函数)，则模型就叫逻辑回归(Logistic Regression)")]),t._v(" "),a("p",[t._v("$g(z)=\\frac{1}{1+e^{-z}}$")]),t._v(" "),a("p",[t._v("于是，整个模型就是 $h_{\\theta}(x)=\\frac{1}{1+e^{-\\theta^Tx}}$")]),t._v(" "),a("p",[t._v("由于逻辑回归的输出为(0, 1)空间，所以可以解释为概率值，例：如果 $h_\\theta{(x)}=0.7$ 则一个病人的肿瘤为恶性的概率为70%。")]),t._v(" "),a("h2",{attrs:{id:"_6-3-决策边界的线性与非线性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-3-决策边界的线性与非线性"}},[t._v("#")]),t._v(" 6.3 决策边界的线性与非线性")]),t._v(" "),a("p",[t._v("当输入g(z)的"),a("strong",[t._v("模型函数为线性函数")]),t._v("时，"),a("strong",[t._v("决策边界就是线性")]),t._v("的。例如，输入线性函数情况下定义决策边界为0.5，则当 $h_\\theta{(x)}\\geq0.5$ 时，$\\theta^Tx$ 就 $\\geq0$，此时 $\\theta^Tx$ 就是二维平面内划分正负样例的一条直线：")]),t._v(" "),a("p",[a("img",{attrs:{src:e(352),alt:"image-20200809201240355"}})]),t._v(" "),a("p",[t._v("当输入g(z)的"),a("strong",[t._v("函数为非线性函数")]),t._v("时，"),a("strong",[t._v("决策边界就可以是非线性")]),t._v("的。例如，当输入函数为$z{(x)}=\\theta_0+\\theta_1{x_1}+\\theta_2{x_2}+\\theta_3{x_1^2}+\\theta_4{x_2^2}$ 时，函数为抛物面，")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=2532408450,1088232931&fm=26&gp=0.jpg",alt:"img"}})]),t._v(" "),a("p",[t._v("此时的"),a("strong",[t._v("决策边界")]),t._v("就可以是一个"),a("strong",[t._v("圆/椭圆")]),t._v("：")]),t._v(" "),a("p",[a("img",{attrs:{src:e(353),alt:"image-20200809201847083"}})]),t._v(" "),a("h2",{attrs:{id:"_6-4-逻辑回归损失函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-4-逻辑回归损失函数"}},[t._v("#")]),t._v(" 6.4 逻辑回归损失函数")]),t._v(" "),a("p",[t._v("逻辑回归的损失函数如下：")]),t._v(" "),a("p",[t._v("$Cost(h_\\theta{(x)},y)=\\begin{cases}-log(h_\\theta(x))&if;y=1\\-log(1-h_\\theta(x))&if;y=0\\end{cases}$")]),t._v(" "),a("p",[t._v("其图像如下：")]),t._v(" "),a("p",[a("img",{attrs:{src:e(354),alt:"image-20200809204653140"}})]),t._v(" "),a("h2",{attrs:{id:"_6-5-简化损失函数与梯度下降"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-5-简化损失函数与梯度下降"}},[t._v("#")]),t._v(" 6.5 简化损失函数与梯度下降")]),t._v(" "),a("p",[a("strong",[t._v("简化后的损失函数")]),t._v("为：")]),t._v(" "),a("p",[t._v("$J(\\theta)=-\\frac{1}{m}[\\sum_{i=1}^{m}y^{(i)}\\log{h_\\theta{(x^{(i)})}}+(1-y^{(i)})\\log{(1-h_\\theta{(x^{(i)})})}]$")]),t._v(" "),a("p",[t._v("同时考虑了y=1和y=0的情况，并且把负号提到了最前面。")]),t._v(" "),a("blockquote",[a("p",[t._v("编者注：这个损失函数就叫做"),a("strong",[t._v("Log损失")]),t._v("或者"),a("strong",[t._v("Logistic损失")]),t._v("或者"),a("strong",[t._v("Logit损失")]),t._v("或者"),a("strong",[t._v("Logarithmic损失")]),t._v("，都是同一个东西。参考: "),a("a",{attrs:{href:"https://stats.stackexchange.com/a/224135/291675",target:"_blank",rel:"noopener noreferrer"}},[t._v("Reference"),a("OutboundLink")],1)])]),t._v(" "),a("p",[a("strong",[t._v("逻辑回归的梯度下降")]),t._v("：")]),t._v(" "),a("p",[t._v("对$J(\\theta)$求导，可得：$\\frac{\\part}{\\part\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}({x^{(i)})}-y^{(i)})\\cdot{x_{j}^{(i)}}$")]),t._v(" "),a("p",[t._v("因此如果要最小化 $J(\\theta)$，算法为：")]),t._v(" "),a("p",[t._v("重复 $\\theta_j:=\\theta_j-\\alpha\\sum_{i=1}^{m}(h_{\\theta}({x^{(i)})}-y^{(i)})\\cdot{x_{j}^{(i)}}$ 直到"),a("strong",[t._v("函数收敛")]),t._v("。")]),t._v(" "),a("p",[t._v("可以发现"),a("strong",[t._v("这个表达式和最小二乘法的线性回归几乎一样")]),t._v("！")]),t._v(" "),a("h2",{attrs:{id:"_6-6-高级优化算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-6-高级优化算法"}},[t._v("#")]),t._v(" 6.6 高级优化算法")]),t._v(" "),a("p",[t._v("除了梯度下降，还有一些更高级的优化算法，例如：")]),t._v(" "),a("ul",[a("li",[t._v("共轭梯度法 ("),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Conjugate_gradient_method",target:"_blank",rel:"noopener noreferrer"}},[t._v("Conjugate Gradient"),a("OutboundLink")],1),t._v(")")]),t._v(" "),a("li",[a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm",target:"_blank",rel:"noopener noreferrer"}},[t._v("BFGS"),a("OutboundLink")],1),t._v("，属于拟牛顿法的一种，以四位数学家Broyden, Fletcher, Goldfarb, Shanno的首字母命名")]),t._v(" "),a("li",[a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Limited-memory_BFGS",target:"_blank",rel:"noopener noreferrer"}},[t._v("L-BFGS"),a("OutboundLink")],1),t._v("，即Limited BFGS，是在有限内存中求BFGS的一种方法")])]),t._v(" "),a("p",[t._v("这些方法"),a("strong",[t._v("无需设定学习率")]),t._v("，一般"),a("strong",[t._v("比梯度下降更快")]),t._v("，但缺点是"),a("strong",[t._v("计算复杂度更高")])]),t._v(" "),a("p",[t._v("在Octave中可以用fminunc函数来使用这些优化算法：")]),t._v(" "),a("div",{staticClass:"language-octave extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("options = optimset('GradObj', 'on', 'MaxIter', 100);\ninitialTheta = zeros(2,1);\n   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);\n")])])]),a("h2",{attrs:{id:"_6-7-多分类：一对多"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-7-多分类：一对多"}},[t._v("#")]),t._v(" 6.7 多分类：一对多")]),t._v(" "),a("p",[t._v("对于多分类问题（大于等于3个类别），可以采用一对多方式，对每个类别都训练一个二分类模型（把当前类为1当作正类），然后在预测时把样本输入到每个模型中，最后选择的类别为最大的输出概率所对应的类别：$\\underset{i}{\\operatorname{argmax}}h_\\theta^{(i)}(x)$")])])}),[],!1,null,null,null);_.default=v.exports}}]);