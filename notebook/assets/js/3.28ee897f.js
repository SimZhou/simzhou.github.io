(window.webpackJsonp=window.webpackJsonp||[]).push([[3],{349:function(t,_,a){t.exports=a.p+"assets/img/image-20200809164252817.d35ad667.png"},350:function(t,_,a){t.exports=a.p+"assets/img/image-20200809170201465.defdc5be.png"},351:function(t,_,a){t.exports=a.p+"assets/img/image-20200809171947353.2e0e45a3.png"},372:function(t,_,a){"use strict";a.r(_);var e=a(42),r=Object(e.a)({},(function(){var t=this,_=t.$createElement,e=t._self._c||_;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("本内容按照吴恩达公开课《Machine Learning》的 Lecture Slides 进行分类，每一个H1标题对应一个Lecture Slide，每一个H2标题对应Lecture Slide中的一个小章节。")]),t._v(" "),e("p",[t._v("本内容是课程的简化总结，适合已经了解机器学习基本概念的人进行回顾以及查漏补缺。")]),t._v(" "),e("h1",{attrs:{id:"_2-一元线性回归"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-一元线性回归"}},[t._v("#")]),t._v(" 2 一元线性回归")]),t._v(" "),e("h2",{attrs:{id:"_2-1-模型介绍"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-模型介绍"}},[t._v("#")]),t._v(" 2.1 模型介绍")]),t._v(" "),e("p",[t._v("以房价预测举例，x为房屋面积，y为房价，那么通过训练数据来学习到的模型$h(x)=\\theta_0+\\theta_1\\cdot{x}$（二维平面的一条线）就可以通过面积来预测房价。")]),t._v(" "),e("h2",{attrs:{id:"_2-2-损失函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-损失函数"}},[t._v("#")]),t._v(" 2.2 损失函数")]),t._v(" "),e("p",[t._v("如何学习正确的$\\theta_0$和$\\theta_1$来确定这条线呢？我们希望这条线和样本数据约靠近越好。")]),t._v(" "),e("p",[e("img",{attrs:{src:a(349),alt:"CostFuction"}})]),t._v(" "),e("h2",{attrs:{id:"_2-3-损失函数理解-i"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-损失函数理解-i"}},[t._v("#")]),t._v(" 2.3 损失函数理解 I")]),t._v(" "),e("p",[t._v("损失函数的目的是"),e("strong",[t._v("最小化均方误差")]),t._v("：")]),t._v(" "),e("p",[t._v("$$\nJ(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2\n$$\n其中"),e("strong",[t._v("m")]),t._v("为样本个数，$h_{\\theta}(x)$为模型函数，x(i) y(i)分别对应样本点。")]),t._v(" "),e("h2",{attrs:{id:"_2-4-损失函数理解-ii"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-损失函数理解-ii"}},[t._v("#")]),t._v(" 2.4 损失函数理解 II")]),t._v(" "),e("p",[e("strong",[t._v("模型")]),t._v("：$h(x)=\\theta_0+\\theta_1\\cdot{x}$")]),t._v(" "),e("p",[e("strong",[t._v("参数")]),t._v("：$\\theta_0$，$\\theta_1$")]),t._v(" "),e("p",[e("strong",[t._v("损失函数")]),t._v("：$J(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2$")]),t._v(" "),e("p",[e("strong",[t._v("目标")]),t._v("：通过改变$\\theta_0$，$\\theta_1$来最小化$J(\\theta_0, \\theta_1)$")]),t._v(" "),e("p",[e("img",{attrs:{src:a(350),alt:"image-20200809170201465"}})]),t._v(" "),e("blockquote",[e("p",[t._v("编者注：后来我们知道，最小化损失函数的过程就是一个凸优化问题")])]),t._v(" "),e("h2",{attrs:{id:"_2-5-梯度下降"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-梯度下降"}},[t._v("#")]),t._v(" 2.5 梯度下降")]),t._v(" "),e("p",[t._v("重复下面步骤，直到"),e("strong",[t._v("收敛")]),t._v("：")]),t._v(" "),e("p",[t._v("$\\theta_j := \\theta_j - \\alpha\\frac{\\part}{\\part\\theta_j}J(\\theta_0,\\theta_1)$")]),t._v(" "),e("blockquote",[e("p",[e("strong",[t._v("重点")]),t._v("：一定要一次性计算好所有方向上的梯度，然后一次性更新所有$\\theta$参数。以下步骤是错误的：")]),t._v(" "),e("p",[t._v("$temp0 := \\theta_0 - \\alpha\\frac{\\part}{\\part\\theta_0}J(\\theta_0,\\theta_1)$")]),t._v(" "),e("p",[t._v("$\\theta_0 := temp0$")]),t._v(" "),e("p",[t._v("$temp1 := \\theta_1 - \\alpha\\frac{\\part}{\\part\\theta_1}J(\\theta_0,\\theta_1)$")]),t._v(" "),e("p",[t._v("$\\theta_1 := temp1$")]),t._v(" "),e("p",[t._v("（此举会导致参数按每个梯度方向都更新一次）")])]),t._v(" "),e("p",[t._v("$\\alpha$："),e("strong",[t._v("学习率")]),t._v("。太小会导致学习太慢，太大会导致跳过最低点甚至损失函数发散")]),t._v(" "),e("h2",{attrs:{id:"_2-6-线性回归的梯度下降"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-6-线性回归的梯度下降"}},[t._v("#")]),t._v(" 2.6 线性回归的梯度下降")]),t._v(" "),e("p",[e("strong",[t._v("求导")]),t._v("可得：")]),t._v(" "),e("p",[t._v("$\\frac{\\part}{\\part\\theta_0}J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}({x^{(i)})}-y^{(i)})$")]),t._v(" "),e("p",[t._v("$\\frac{\\part}{\\part\\theta_1}J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}({x^{(i)})}-y^{(i)})\\cdot{x^{(i)}}$")]),t._v(" "),e("p",[e("img",{attrs:{src:a(351),alt:"image-20200809171947353"}})]),t._v(" "),e("p",[e("strong",[t._v("批梯度下降")]),t._v("：对应损失函数中的m，批梯度下降使用所有的训练数据")]),t._v(" "),e("p",[e("strong",[t._v("随机梯度下降")]),t._v("：一次训练不采用全部的训练数据（降低m），目的是减少计算量")])])}),[],!1,null,null,null);_.default=r.exports}}]);