(window.webpackJsonp=window.webpackJsonp||[]).push([[32],{401:function(t,e,r){"use strict";r.r(e);var n=r(42),s=Object(n.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h2",{attrs:{id:"第4节-神经网络及其训练算法介绍"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#第4节-神经网络及其训练算法介绍"}},[t._v("#")]),t._v(" 第4节 神经网络及其训练算法介绍")]),t._v(" "),r("p",[t._v("**补充知识：Lazy Learning and Eager Learning，Outliers, **")]),t._v(" "),r("p",[r("strong",[t._v("Lazy Learning")])]),t._v(" "),r("p",[t._v("​\t\t特点： 简单地把训练样本存储起来，直到需要分类新的实例时才分析其与所存储样例的关系，据此确定新实例的目标函数值。也就是说这种学习方式只有到了需要决策时才会利用已有数据进行决策，而在这之前不会经历 Eager Learning所拥有的训练过程。")]),t._v(" "),r("p",[t._v("​        例子：KNN，Naive Bayers")]),t._v(" "),r("p",[r("strong",[t._v("Eager Learning")])]),t._v(" "),r("p",[t._v("​\t\t特点： 在进行某种判断（例如，确定一个点的分类或者回归中确定某个点对应的函数值）之前，先利用训练数据进行训练得到一个目标函数，待需要时就只利用训练好的函数进行决策，显然这是一种一劳永逸的方法。")]),t._v(" "),r("p",[t._v("​        例子：SVM、LR、Decision Tree、Neural Networks、……")]),t._v(" "),r("p",[r("strong",[t._v("Outliers：异常点")])]),t._v(" "),r("p",[t._v("简单方法：用Percentiles剔除比如20%以外的数据")]),t._v(" "),r("p",[t._v("这一节主要学习了：")]),t._v(" "),r("p",[t._v("神经网络")]),t._v(" "),r("p",[t._v("逻辑回归")]),t._v(" "),r("p",[t._v("Logistic Loss 与 交叉熵的原理与应用（LogitLoss适用于01分布，交叉熵适用于多分类，本质上是一样的）")]),t._v(" "),r("p",[t._v("反向传播、链式法则……")]),t._v(" "),r("p",[r("a",{attrs:{href:"https://github.com/SimZhou/NLP_Assignments",target:"_blank",rel:"noopener noreferrer"}},[t._v("返回目录"),r("OutboundLink")],1)])])}),[],!1,null,null,null);e.default=s.exports}}]);