<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>第10节 机器学习算法（贝叶斯，k-means，SVM，随机森林，XGBoost 等） | Simon’s Notebook</title>
    <meta name="generator" content="VuePress 1.5.2">
    <link rel="icon" href="/notebook/doraemon_winter_circle.png">
    <meta name="description" content="A Technical Notebook - Simon的技术笔记">
    <link rel="preload" href="/notebook/assets/css/0.styles.a59edf66.css" as="style"><link rel="preload" href="/notebook/assets/js/app.505e09e2.js" as="script"><link rel="preload" href="/notebook/assets/js/2.0abb7677.js" as="script"><link rel="preload" href="/notebook/assets/js/11.460c3070.js" as="script"><link rel="prefetch" href="/notebook/assets/js/10.f79ea1b2.js"><link rel="prefetch" href="/notebook/assets/js/12.731964c3.js"><link rel="prefetch" href="/notebook/assets/js/13.95b96f58.js"><link rel="prefetch" href="/notebook/assets/js/14.27e8b3c4.js"><link rel="prefetch" href="/notebook/assets/js/15.4f456819.js"><link rel="prefetch" href="/notebook/assets/js/16.867a2ce1.js"><link rel="prefetch" href="/notebook/assets/js/17.3dae4fcb.js"><link rel="prefetch" href="/notebook/assets/js/18.60c15a1f.js"><link rel="prefetch" href="/notebook/assets/js/19.0aa6db32.js"><link rel="prefetch" href="/notebook/assets/js/20.298a5fd5.js"><link rel="prefetch" href="/notebook/assets/js/21.3dccbb22.js"><link rel="prefetch" href="/notebook/assets/js/22.d2e407eb.js"><link rel="prefetch" href="/notebook/assets/js/23.4083cdec.js"><link rel="prefetch" href="/notebook/assets/js/24.c250d814.js"><link rel="prefetch" href="/notebook/assets/js/25.59a95931.js"><link rel="prefetch" href="/notebook/assets/js/26.6c7a8860.js"><link rel="prefetch" href="/notebook/assets/js/27.a33da1e2.js"><link rel="prefetch" href="/notebook/assets/js/28.c023aa73.js"><link rel="prefetch" href="/notebook/assets/js/29.735deb56.js"><link rel="prefetch" href="/notebook/assets/js/3.474796ef.js"><link rel="prefetch" href="/notebook/assets/js/30.d98e8cac.js"><link rel="prefetch" href="/notebook/assets/js/31.a0ce318a.js"><link rel="prefetch" href="/notebook/assets/js/32.80d83bb1.js"><link rel="prefetch" href="/notebook/assets/js/33.e93abe12.js"><link rel="prefetch" href="/notebook/assets/js/4.4a3319fd.js"><link rel="prefetch" href="/notebook/assets/js/5.30fef6e7.js"><link rel="prefetch" href="/notebook/assets/js/6.41669cbf.js"><link rel="prefetch" href="/notebook/assets/js/7.8267f2fc.js"><link rel="prefetch" href="/notebook/assets/js/8.6cae0a1d.js"><link rel="prefetch" href="/notebook/assets/js/9.3453cee1.js">
    <link rel="stylesheet" href="/notebook/assets/css/0.styles.a59edf66.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/notebook/" class="home-link router-link-active"><img src="/notebook/doraemon_winter_circle.png" alt="Simon’s Notebook" class="logo"> <span class="site-name can-hide">Simon’s Notebook</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/notebook/guide/" class="nav-link">
  导航
</a></div><div class="nav-item"><a href="/notebook/resource/" class="nav-link">
  资源
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="前端" class="dropdown-title"><span class="title">前端</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/frontend/bugfix-record/" class="nav-link">
  Vue踩坑记录
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><span class="title">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/algo/data-structure-and-algo/" class="nav-link">
  算法与数据结构
</a></li><li class="dropdown-item"><!----> <a href="/notebook/algo/machine-learning/" class="nav-link">
  机器学习
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="/cs-basics/" class="dropdown-title"><span class="title">计算机基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/cs-basics/network/" class="nav-link">
  计算机网络
</a></li></ul></div></div><div class="nav-item"><a href="/notebook/reading/" class="nav-link">
  阅读
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/notebook/guide/" class="nav-link">
  导航
</a></div><div class="nav-item"><a href="/notebook/resource/" class="nav-link">
  资源
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="前端" class="dropdown-title"><span class="title">前端</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/frontend/bugfix-record/" class="nav-link">
  Vue踩坑记录
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><span class="title">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/algo/data-structure-and-algo/" class="nav-link">
  算法与数据结构
</a></li><li class="dropdown-item"><!----> <a href="/notebook/algo/machine-learning/" class="nav-link">
  机器学习
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="/cs-basics/" class="dropdown-title"><span class="title">计算机基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/cs-basics/network/" class="nav-link">
  计算机网络
</a></li></ul></div></div><div class="nav-item"><a href="/notebook/reading/" class="nav-link">
  阅读
</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="第10节-机器学习算法（贝叶斯，k-means，svm，随机森林，xgboost-等）"><a href="#第10节-机器学习算法（贝叶斯，k-means，svm，随机森林，xgboost-等）" class="header-anchor">#</a> 第10节 机器学习算法（贝叶斯，k-means，SVM，随机森林，XGBoost 等）</h2> <p>这一节主要学习了：经典机器学习中的贝叶斯方法，K-means，SVM；集成学习中的随机森林，XGBoost</p> <h3 id="_1-贝叶斯方法"><a href="#_1-贝叶斯方法" class="header-anchor">#</a> 1. 贝叶斯方法</h3> <blockquote><p>Recap of probability theory:</p> <p>Independency Assumption:<img src="http://latex.codecogs.com/gif.latex?P(A%5Ccap%7BB%7D)=P(A)%5Ccdot%7BP(B)%7D" alt=""></p> <p>Rule of Total Probability: <img src="http://latex.codecogs.com/gif.latex?P(A)=%5Csum_i%7BP(A%7CB_i)%5Ccdot%7BP(B_i)%7D%7D" alt=""></p> <p>Bayes' Rule: <img src="http://latex.codecogs.com/gif.latex?P(A%7CB)=%5Cfrac%7BP(A%5Ccap%7BB%7D)%7D%7BP(B)%7D=%5Cfrac%7BP(B%7CA)%5Ccdot%7BP(A)%7D%7D%7BP(B)%7D" alt=""></p></blockquote> <p>先验概率：<em>指根据以往经验和分析得到的概率</em></p> <p>后验概率：<em>事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小</em></p> <p><img src="http://latex.codecogs.com/gif.latex?P(A%7CB)=%5Cfrac%7BP(A%5Ccap%7BB%7D)%7D%7BP(B)%7D=%5Cfrac%7BP(B%7CA)%5Ccdot%7BP(A)%7D%7D%7BP(B)%7D" alt=""></p> <h4 id="_1-1-maximum-a-posterior（极大后验概率）、maximum-likelihood（极大似然估计）"><a href="#_1-1-maximum-a-posterior（极大后验概率）、maximum-likelihood（极大似然估计）" class="header-anchor">#</a> 1.1 Maximum a Posterior（极大后验概率）、Maximum Likelihood（极大似然估计）</h4> <p><img src="http://latex.codecogs.com/gif.latex?h_%7BMAP%7D=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7DP(h%7CD)=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7DP(D%7Ch)P(h)" alt=""></p> <p><img src="http://latex.codecogs.com/gif.latex?h_%7BML%7D=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7DP(D%7Ch)" alt=""></p> <p>where D is short for Data, H is the set of Hypothesises, h is a particular function in the hypothesises</p> <p>简单解释：</p> <ul><li>在极大后验概率情况下，Data已经发生，我们需要求一个最佳h，它对数据的拟合情况最好（生成概率最大），则这个h就是最佳h</li> <li>在极大似然的情况下，我们要求一个h，使得在这个h下，我们取得观测数据D的概率最大，则这个h是最佳h</li></ul> <h4 id="_1-2-朴素贝叶斯-naive-bayes"><a href="#_1-2-朴素贝叶斯-naive-bayes" class="header-anchor">#</a> 1.2 朴素贝叶斯 Naive Bayes</h4> <p>朴素贝叶斯分类器，是一个生成模型，对于每一种分类，它都可以生成一个后验概率</p> <p><img src="http://latex.codecogs.com/gif.latex?h_%7By=y_0,%7BMAP%7D%7D%5C=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7DP(y=y_0%7Ca_1,a_2,a_3,%5Ccdots)%5Cnewline=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7D%5Cfrac%7BP(a_1,a_2,a_3,%5Ccdots%7Cy=y_0)%5Ccdot%7BP(y=y_0)%7D%7D%7BP(a_1,a_2,%5Ccdots)%7D%5Cnewline=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7D%7BP(a_1,a_2,a_3,%5Ccdots%7Cy=y_0)%5Ccdot%7BP%7D(y=y_0)%7D%5Cnewline=%5Carg%5Cmax_%7Bh%5Cin%7BH%7D%7D%7BP(a_1%7Cy_0)%5Ccdot%7BP(a_2%7Cy_0)%7D%5Ccdot%7BP(a_3%7Cy_0)%7D%5Ccdots%7BP(y=y_0)%7D%7D" alt=""></p> <p>(Last step according to &quot;Naive&quot; hypothesis)</p> <p>然后我们同样可以求出<img src="http://latex.codecogs.com/gif.latex?h_%7By=y_1%7D" alt="">, <img src="http://latex.codecogs.com/gif.latex?h_%7By=y_2%7D" alt="">, ...。最后y的估计值就是使h得到最大值所对于的y值。</p> <p>例：文档分类</p> <p><img src="http://uricc.ga/images/2019/12/13/_20191213164304.png" alt=""></p> <p>在用朴素贝叶斯进行文档分类的例子中，由于词库中某些词可能在句子中没有出现，产生条件概率为0的情况，因此对于所有词的初始词频默认为1（而不是0）（Add 1 smoothing）</p> <h5 id=""><a href="#" class="header-anchor">#</a></h5> <h3 id="_2-k-means"><a href="#_2-k-means" class="header-anchor">#</a> 2. K-means</h3> <blockquote><p><strong>Input:</strong> dataset D, number of clusters K</p> <p><strong>Output:</strong> partition of D into k clusters</p> <p><strong>Algorithm:</strong></p> <p>​		Choose k random seeds</p> <p>​		Repeat until no changes:</p> <p>​				Assign each instance to the cluster of its closest seed</p> <p>​				Redefine seeds as cluster means</p> <p>​		Return k clusters</p></blockquote> <p>如何提升聚类效果：k个点的选取尽可能互相远离</p> <h3 id="_3-svm"><a href="#_3-svm" class="header-anchor">#</a> 3. SVM</h3> <blockquote><p>前置知识：拉格朗日乘子法，用于解决条件约束优化问题。<br></p> <p><strong>Equility constraints:</strong></p> <p>​	<img src="http://latex.codecogs.com/gif.latex?%5Cmin_%7Bx%5Cin%7BR%7D%7Df(x)%5Cquad%5Ctext%7Bs.t.%7Dh(x)=0" alt=""></p> <p>​	令<img src="http://latex.codecogs.com/gif.latex?L=f(x)+%5Clambda%5Ccdot%7Bh(x)%7D" alt="">,</p> <p>​	然后L对 x 和 λ 分别求偏导并令它们都等于 0，即可得到最优解</p> <p>​	<strong>实际意义为：对x求偏导为零，即等同于切线平行：<img src="http://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Cpartial%7Bf%7D%7D%7B%5Cpartial%7Bx%7D%7D=-%5Clambda%5Ccdot%5Cfrac%7B%5Cpartial%7Bh%7D%7D%7B%5Cpartial%7Bx%7D%7D" alt=""></strong></p> <p>​    					   <strong>对λ求偏导，即等同于满足约束条件：<img src="http://latex.codecogs.com/gif.latex?h(x)=0" alt=""></strong></p> <p>​	详解见3B1B youtube视频：<a href="https://www.youtube.com/embed/yuqB-d5MjZA" target="_blank" rel="noopener noreferrer">Lagrange multipliers, using tangency to solve constrained optimization<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><strong>Inequility constrains:</strong></p> <p>​	<img src="http://latex.codecogs.com/gif.latex?%5Cmin_%7Bx%5Cin%7BR%7D%7Df(x)%5Cquad%5Ctext%7Bs.t.%7Dg(x)%5Cleq0" alt=""></p> <p>​	令<img src="http://latex.codecogs.com/gif.latex?L=f(x)+%5Clambda%5Ccdot%7Bg(x)%7D" alt=""></p> <p>​    若最小值点就在约束范围内，则约束条件相当于没有用，</p> <p>​    若最小值点在约束范围边界上，则约束条件起作用：</p> <p>​    	f与g梯度方向相反：<img src="http://latex.codecogs.com/gif.latex?-%5Cnabla%7Bf(x)%7D=u%5Ccdot%5Cnabla%7Bg(x)%7D" alt=""> （即求偏导）且  <img src="http://latex.codecogs.com/gif.latex?u%5Cge%7B0%7D" alt=""></p> <p>​        约束条件：<img src="http://latex.codecogs.com/gif.latex?g(x)%5Cle%7B0%7D" alt=""></p> <p>​		<img src="http://latex.codecogs.com/gif.latex?u%5Ccdot%7Bg(x)%7D=0" alt=""> （u=0，则最小值点在约束范围内，约束不成立；g(x)=0 则约束条件成立）</p> <p><strong>KKT条件：</strong></p> <p>​	<img src="http://latex.codecogs.com/gif.latex?%5Cmin_%7Bx%5Cin%7BR%7D%7Df(x)%5Cquad%5Ctext%7Bs.t.%7Df(x)=0,%5Cquad%7Bg(x)%7D%5Cleq0" alt=""></p> <p>​	把等式约束和不等式约束加到一起，则满足以下条件的点就是极值点：</p> <p>​		<img src="http://latex.codecogs.com/gif.latex?%5Cnabla_%7Bx%7DL=0" alt=""></p> <p>​		<img src="http://latex.codecogs.com/gif.latex?%5Cmu%5Ccdot%7Bg(x)%7D=0" alt=""></p> <p>​		<img src="http://latex.codecogs.com/gif.latex?h(x)=0" alt=""></p> <p>​		<img src="http://latex.codecogs.com/gif.latex?g(x)%5Cleq%7B0%7D" alt=""></p> <p>​		<img src="http://latex.codecogs.com/gif.latex?%5Cmu%5Cgeq%7B0%7D%7D" alt=""></p> <p>(这一部分还需消化理解，详见 <em>Convex Optimization</em> 第6章：对偶问题，以及博客：<a href="https://blog.csdn.net/fkyyly/article/details/86488582" target="_blank" rel="noopener noreferrer">link<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>)</p></blockquote> <p>SVM 的目的是最大化间隔，即</p> <h4 id="kernel-trick"><a href="#kernel-trick" class="header-anchor">#</a> Kernel Trick</h4> <p>使用非线性函数将数据打到高维空间，使之在高维空间线性可分：</p> <p><img src="http://uricc.ga/images/2019/12/20/_20191220115256.png" alt="Kernal Trick"></p> <h4 id="svm的优缺点"><a href="#svm的优缺点" class="header-anchor">#</a> SVM的优缺点</h4> <p>优点：</p> <ul><li><p>凸优化给予<strong>全局最优</strong></p></li> <li><p>用<strong>核技巧</strong>可以处理<strong>非线性数据</strong></p></li> <li><p>维度灾难还好 https://www.quora.com/Does-SVM-suffer-from-the-curse-of-dimensionality-If-so-how-does-SVM-overcome-it</p></li> <li><p>可解释</p></li></ul> <p>缺点：</p> <ul><li>计算复杂度高（需要遍历所有样本点）</li> <li>本质上是二分类模型，对多分类问题效果不会特别好（尽管有技巧让它可以用于多分类例如one-to-many）</li></ul> <h3 id="_4-集成学习（ensemble-learning）"><a href="#_4-集成学习（ensemble-learning）" class="header-anchor">#</a> 4. 集成学习（Ensemble Learning）</h3> <blockquote><p>集成学习的思想是训练多个模型，然后通过投票的方式获取多数模型赞同的预测，以获得更好的结果。</p> <p>集成学习的分类有：</p> <p><strong>Bagging：每次通过Bootstrap方式训练一个分类器，各分类器相互独立</strong></p> <p><img src="http://uricc.ga/images/2019/12/20/bagging.png" alt=""></p> <p><strong>Boosting：每次训练出的模型的学习效果不好的部分，都会提升下一次模型的效果</strong></p> <p><img src="http://uricc.ga/images/2019/12/20/boosting.png" alt=""></p></blockquote> <h4 id="_4-1-随机森林"><a href="#_4-1-随机森林" class="header-anchor">#</a> 4.1 随机森林</h4> <p>随机森林算法的过程：</p> <blockquote><p>训练集 S := (x1, y1),...,(xn, yn)，特征空间 F，决策树数目 B.</p> <p><strong>function RandomForest(S, F)</strong></p> <p>​		H ← 0</p> <p>​		<strong>for</strong> i ∈ 1, ..., B <strong>do</strong></p> <p>​				S(i) ← A bootstrap sample from S</p> <p>​				h(i) ← RandomizedTreeLearn(S(i), F)</p> <p>​				H ← H ∪ {h(i)}</p> <p>​		<strong>end for</strong></p> <p>​		<strong>return H</strong></p> <p><strong>end function</strong></p> <p><strong>function RandomizedTreeLearn(S, F)</strong></p> <p>​		At each node:</p> <p>​				f ← very small subset of F</p> <p>​				Split on best feature in f</p> <p>​		<strong>return The learned tree</strong></p> <p><strong>end function</strong></p></blockquote> <p>随机的体现：</p> <ul><li>样本随机取得（bootstrap有放回抽样）</li> <li>特征的随机取得（每棵树只选取一部分特征，比如t个特征）</li></ul> <h4 id="_4-2-xgboost"><a href="#_4-2-xgboost" class="header-anchor">#</a> 4.2 XGBoost</h4> <blockquote><p>预备知识：回归树</p> <p>​	test样本的y值，取的是最后叶子节点中train样本的y值的平均；</p> <p>​	选择分类点时，用的不是信息增益，而是最小化类间方差</p></blockquote> <p><strong>目标函数：</strong><img src="http://latex.codecogs.com/gif.latex?Obj=%5Csum_%7Bi=1%7D%5E%7Bn%7Dl(y_i,%5Chat%7By%7D_i)+%5Csum_%7Bk=1%7D%5E%7BK%7D%5COmega(f_k)" alt=""></p> <p>其中，<img src="http://latex.codecogs.com/gif.latex?l" alt="">为loss；<img src="http://latex.codecogs.com/gif.latex?%5COmega" alt="">为正则化项，表示树的复杂度函数。</p> <p>其中 <img src="http://latex.codecogs.com/gif.latex?%5COmega(f)=%5Cgamma%7BT%7D+%5Cfrac%7B1%7D%7B2%7D%5Clambda%5Csum_%7Bj=1%7D%5E%7BT%7Dw_j%5E%7B2%7D" alt="">，<img src="http://latex.codecogs.com/gif.latex?T" alt="">代表叶子节点的个数，<img src="http://latex.codecogs.com/gif.latex?w" alt="">代表叶子节点的分数，分别由<img src="http://latex.codecogs.com/gif.latex?%5Cgamma" alt="">和<img src="http://latex.codecogs.com/gif.latex?%5Clambda" alt="">控制。</p> <p>参考：<a href="https://www.cnblogs.com/zongfa/p/9324684.html" target="_blank" rel="noopener noreferrer">机器学习--boosting家族之XGBoost算法<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>，<a href="https://blog.csdn.net/zww275250/article/details/78652522" target="_blank" rel="noopener noreferrer">XGBoost 的前世今生<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>，<a href="https://blog.csdn.net/hemeinvyiqiluoben/article/details/87870656" target="_blank" rel="noopener noreferrer">COS 访谈第 18 期：陈天奇<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>，<a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" target="_blank" rel="noopener noreferrer">XGBoost官方文档<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>，<a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.md" target="_blank" rel="noopener noreferrer">NLP-LOVE的XGBoost介绍<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/notebook/assets/js/app.505e09e2.js" defer></script><script src="/notebook/assets/js/2.0abb7677.js" defer></script><script src="/notebook/assets/js/11.460c3070.js" defer></script>
  </body>
</html>
