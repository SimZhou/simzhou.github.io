<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>第16节 深度学习高阶知识——强化学习 | Simon’s Notebook</title>
    <meta name="generator" content="VuePress 1.7.1">
    <link rel="icon" href="/notebook/doraemon_winter_circle.png">
    <meta name="description" content="A Technical Notebook - Simon的技术笔记">
    
    <link rel="preload" href="/notebook/assets/css/0.styles.45fc59ad.css" as="style"><link rel="preload" href="/notebook/assets/js/app.ecc66517.js" as="script"><link rel="preload" href="/notebook/assets/js/2.4b33a2ac.js" as="script"><link rel="preload" href="/notebook/assets/js/38.cc1cb47e.js" as="script"><link rel="prefetch" href="/notebook/assets/js/10.feba53be.js"><link rel="prefetch" href="/notebook/assets/js/11.5122d181.js"><link rel="prefetch" href="/notebook/assets/js/12.310db791.js"><link rel="prefetch" href="/notebook/assets/js/13.5056b923.js"><link rel="prefetch" href="/notebook/assets/js/14.bb27e4d0.js"><link rel="prefetch" href="/notebook/assets/js/15.caa67b98.js"><link rel="prefetch" href="/notebook/assets/js/16.369b933e.js"><link rel="prefetch" href="/notebook/assets/js/17.3d26d7c2.js"><link rel="prefetch" href="/notebook/assets/js/18.c299f329.js"><link rel="prefetch" href="/notebook/assets/js/19.fd60f9aa.js"><link rel="prefetch" href="/notebook/assets/js/20.cbbcc6ea.js"><link rel="prefetch" href="/notebook/assets/js/21.639ba462.js"><link rel="prefetch" href="/notebook/assets/js/22.8c8810a8.js"><link rel="prefetch" href="/notebook/assets/js/23.a4c01d00.js"><link rel="prefetch" href="/notebook/assets/js/24.9f653f28.js"><link rel="prefetch" href="/notebook/assets/js/25.8e361fe6.js"><link rel="prefetch" href="/notebook/assets/js/26.722a049d.js"><link rel="prefetch" href="/notebook/assets/js/27.448c3b28.js"><link rel="prefetch" href="/notebook/assets/js/28.e0ca1d37.js"><link rel="prefetch" href="/notebook/assets/js/29.8c6af69a.js"><link rel="prefetch" href="/notebook/assets/js/3.b4cda513.js"><link rel="prefetch" href="/notebook/assets/js/30.cff2a7e0.js"><link rel="prefetch" href="/notebook/assets/js/31.3509c662.js"><link rel="prefetch" href="/notebook/assets/js/32.cffa5419.js"><link rel="prefetch" href="/notebook/assets/js/33.c973aa58.js"><link rel="prefetch" href="/notebook/assets/js/34.07acd3a0.js"><link rel="prefetch" href="/notebook/assets/js/35.121e5d2d.js"><link rel="prefetch" href="/notebook/assets/js/36.7449ec8b.js"><link rel="prefetch" href="/notebook/assets/js/37.eaabadaf.js"><link rel="prefetch" href="/notebook/assets/js/39.0720172f.js"><link rel="prefetch" href="/notebook/assets/js/4.b0cb8dee.js"><link rel="prefetch" href="/notebook/assets/js/40.3a910db2.js"><link rel="prefetch" href="/notebook/assets/js/41.08f177fc.js"><link rel="prefetch" href="/notebook/assets/js/42.c3b54c25.js"><link rel="prefetch" href="/notebook/assets/js/43.5a2c9e0b.js"><link rel="prefetch" href="/notebook/assets/js/44.4ecc7007.js"><link rel="prefetch" href="/notebook/assets/js/45.1648a80a.js"><link rel="prefetch" href="/notebook/assets/js/46.ce7abc51.js"><link rel="prefetch" href="/notebook/assets/js/5.e1d75be5.js"><link rel="prefetch" href="/notebook/assets/js/6.edaa62b8.js"><link rel="prefetch" href="/notebook/assets/js/7.28e9c996.js"><link rel="prefetch" href="/notebook/assets/js/8.0cec774e.js"><link rel="prefetch" href="/notebook/assets/js/9.ed7a0d89.js">
    <link rel="stylesheet" href="/notebook/assets/css/0.styles.45fc59ad.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/notebook/" class="home-link router-link-active"><img src="/notebook/doraemon_winter_circle.png" alt="Simon’s Notebook" class="logo"> <span class="site-name can-hide">Simon’s Notebook</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/notebook/guide/" class="nav-link">
  导航
</a></div><div class="nav-item"><a href="/notebook/resource/" class="nav-link">
  资源
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><span class="title">算法</span> <span class="arrow down"></span></button> <button type="button" aria-label="算法" class="mobile-dropdown-title"><span class="title">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/algo/" class="nav-link">
  算法导航页
</a></li><li class="dropdown-item"><h4>
          算法与数据结构
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/algo/ds-algo/pending.html" class="nav-link">
  算法与数据结构
</a></li><li class="dropdown-subitem"><a href="/notebook/algo/ds-algo/leetcode/" class="nav-link">
  Leetcode刷题总结
</a></li></ul></li><li class="dropdown-item"><h4>
          机器学习
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/algo/machine-learning/ml_big_map.html" class="nav-link">
  机器学习算法
</a></li><li class="dropdown-subitem"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-01.html" class="nav-link">
  吴恩达《机器学习公开课》笔记
</a></li></ul></li></ul></div></div><div class="nav-item"><a href="/notebook/nlp/" class="nav-link router-link-active">
  NLP
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="/cs-basics/" class="dropdown-title"><span class="title">计算机基础</span> <span class="arrow down"></span></button> <button type="button" aria-label="/cs-basics/" class="mobile-dropdown-title"><span class="title">计算机基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/cs-basics/network/" class="nav-link">
  计算机网络
</a></li></ul></div></div><div class="nav-item"><a href="/notebook/reading/" class="nav-link">
  阅读
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/notebook/guide/" class="nav-link">
  导航
</a></div><div class="nav-item"><a href="/notebook/resource/" class="nav-link">
  资源
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><span class="title">算法</span> <span class="arrow down"></span></button> <button type="button" aria-label="算法" class="mobile-dropdown-title"><span class="title">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/algo/" class="nav-link">
  算法导航页
</a></li><li class="dropdown-item"><h4>
          算法与数据结构
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/algo/ds-algo/pending.html" class="nav-link">
  算法与数据结构
</a></li><li class="dropdown-subitem"><a href="/notebook/algo/ds-algo/leetcode/" class="nav-link">
  Leetcode刷题总结
</a></li></ul></li><li class="dropdown-item"><h4>
          机器学习
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/algo/machine-learning/ml_big_map.html" class="nav-link">
  机器学习算法
</a></li><li class="dropdown-subitem"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-01.html" class="nav-link">
  吴恩达《机器学习公开课》笔记
</a></li></ul></li></ul></div></div><div class="nav-item"><a href="/notebook/nlp/" class="nav-link router-link-active">
  NLP
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="/cs-basics/" class="dropdown-title"><span class="title">计算机基础</span> <span class="arrow down"></span></button> <button type="button" aria-label="/cs-basics/" class="mobile-dropdown-title"><span class="title">计算机基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/cs-basics/network/" class="nav-link">
  计算机网络
</a></li></ul></div></div><div class="nav-item"><a href="/notebook/reading/" class="nav-link">
  阅读
</a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/notebook/nlp/nlp_big_map.html" class="sidebar-link">NLP Big Map</a></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>NLP Notes</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/notebook/nlp/" aria-current="page" class="sidebar-link">NLP</a></li><li><a href="/notebook/nlp/Assignment01/" class="sidebar-link">1.基于语法树和概率的模型</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="第16节-深度学习高阶知识-强化学习"><a href="#第16节-深度学习高阶知识-强化学习" class="header-anchor">#</a> 第16节 深度学习高阶知识——强化学习</h2> <p>这一节学了：强化学习。</p> <p>目录：</p> <ul><li>概念</li> <li>Bellman等式</li> <li>动态规划</li> <li>蒙特卡洛</li> <li>TD</li> <li>Q-learning 和 SARSA</li> <li>Deep Q-Network 和 Policy gradient</li></ul> <h3 id="_1-强化学习概念"><a href="#_1-强化学习概念" class="header-anchor">#</a> 1. 强化学习概念</h3> <p>强化学习是一个 Agent 与 Environment 相互交互的过程</p> <p>​	   state s∈<strong>S</strong></p> <p>​      action a∈<strong>A</strong></p> <p>​     ——————</p> <p>​    |					 ↓</p> <p>Agent		Environment</p> <p>​	↑					 |</p> <p>​     ——————</p> <p>​	  Get reward <strong>r</strong></p> <p>​    New state s'∈<strong>S</strong></p> <p>其中重要的概念有如下：</p> <ul><li><p>States状态空间：包含所有可能的状态</p></li> <li><p>Actions动作空间：包含所有可能的动作</p></li> <li><p>Rewards奖励：R（s, a）在不同的状态采取相同或不同的动作都有对应的一个奖励</p></li> <li><p>Values期望值：总奖励</p></li> <li><p>Policy：策略，代表在某个状态s的最佳动作action</p></li> <li><p>Episode：通过一系列动作经过一系列状态后，获得的一个序列，例如：S1 A1 R2 S2 A2 R3 S3 …… ST（终止）</p></li></ul> <p><strong>Model Transition（数学描述状态转移）</strong></p> <p><img src="http://latex.codecogs.com/gif.latex?P(S%5E%7B'%7D,r%7CS,a)=P%5BS_%7Bt+1%7D=s%5E%7B'%7D,R_%7Bt+1%7D=r%7CS_t=s,A_t=a%5D=%5Csum_%7Br%5Cin%7BR%7D%7DP(s%5E%7B'%7D,r%7CS,a)" alt=""></p> <p>期望Reward: <img src="http://latex.codecogs.com/gif.latex?R(s,a)=E%5BR_%7Bt+1%7D%7CS_t,A_t=a%5D" alt=""></p> <p><strong>Value Function and action-value(Q-value) （数学描述奖励函数）</strong></p> <p>Rewards: <img src="http://latex.codecogs.com/gif.latex?G_t=R_%7Bt+1%7D+%5Cgamma%7BR_%7Bt+2%7D%7D+%5Cgamma%5E%7B2%7D%7BR_%7Bt+3%7D%7D+%5Cgamma%5E%7B3%7D%7BR_%7Bt+4%7D%7D+%5Ccdots" alt=""></p> <p>State values: <img src="http://latex.codecogs.com/gif.latex?V_%5Cpi(s)=E_%5Cpi%5BG_t%7CS_t=s%5D" alt="">，是在某个状态s下，Rewards的期望值。适用于路线不唯一且具有随机性的步骤。</p> <p>Q-values：<img src="http://latex.codecogs.com/gif.latex?Q_%5Cpi(s,a)=E_%5Cpi%5BG_t%7CS_t=s,A_t=a%5D" alt="">，是在某个状态s下，已经采取动作a后，Rewards的期望值。</p> <p>A-values：<img src="http://latex.codecogs.com/gif.latex?Q_%5Cpi(s,a)-V_%5Cpi(s)" alt="">，代表了采取步骤后，所获得的奖励与平均值的差值，如果它高于平均值（A-value＞0），那么该动作应该来讲是比较好的选择。</p> <h3 id="_2-bellman等式-奖励函数的计算方法"><a href="#_2-bellman等式-奖励函数的计算方法" class="header-anchor">#</a> 2. Bellman等式 - 奖励函数的计算方法</h3> <p>Bellman equation是求解每个状态s的V值的过程，只要求解出每个状态的V值后，我们就可以通过每次都选择最大V值来获得最优策略。</p> <p><strong>Bellman equation (1):</strong></p> <p><img src="http://latex.codecogs.com/gif.latex?%5Cbegin%7Baligned%7DV(s)&amp;=E%5BG_t%7CS_t=s%5D%5C&amp;=E%5BR_%7Bt+1%7D+%5Cgamma%7BR_%7Bt+2%7D%7D+%5Cgamma%5E%7B2%7D%7BR_%7Bt+3%7D%7D+%5Ccdots%7CS_t=s%5D%5C&amp;=E%5BR_%7Bt+1%7D+%5Cgamma(%7BR_%7Bt+2%7D%7D+%5Cgamma%7BR_%7Bt+3%7D%7D+%5Ccdots)%7CS_t=s%5D%5C&amp;=E%5BR_%7Bt+1%7D+%5Cgamma%7BG_%7Bt+1%7D%7CS_t=s%7D%5D%5C&amp;=E%5BR_%7Bt+1%7D+%5Cgamma%7BV(S_%7Bt+1%7D)%7D%7CS_t=s%5D%5Cend%7Baligned%7D" alt=""></p> <p>可以看到，这是一个递归，所以要求解V(s)，只需要从末端反推就可以。</p> <p><strong>Bellman equation (2):</strong></p> <p><img src="http://latex.codecogs.com/gif.latex?%5Cbegin%7Baligned%7DQ(s,a)&amp;=E%5BR_%7Bt+1%7D+%5Cgamma%7BV(S_%7Bt+1%7D)%7D%7CS_t=s,A_t=a%5D%5C&amp;=E%5BR_%7Bt+1%7D+%5Cgamma%7BE_%7Ba=%5Cpi%7D%5B%7BQ(s_%7Bt+1%7D,a)%5D%7D%7D%7CS_t=s,A_t=a%5D%5Cend%7Baligned%7D" alt=""></p> <p>通过求解max(Qa_1, Qa_2, Qa_3,...)来获得当前步最佳策略action。</p> <p><img src="http://uricc.ga/images/2020/02/06/_20200207001807.png" alt=""></p> <h3 id="_3-dynamic-programming-奖励函数的求解方法"><a href="#_3-dynamic-programming-奖励函数的求解方法" class="header-anchor">#</a> 3. Dynamic Programming - 奖励函数的求解方法</h3> <p><strong>Value iteration 值迭代</strong></p> <p>具体做法是：先对所有状态初始化一个值，然后使用Bellman equation<img src="http://latex.codecogs.com/gif.latex?V(S_t)=r+%5Csum_%7BP%7D%7BP(S_%7Bt+1%7D%7CS_t)%7DV(S_%7Bt+1%7D)" alt="">进行值的更新，然后不断迭代，直到收敛 |W_t+1 - W_t| ≤ δ：</p> <p><img src="http://uricc.ga/images/2020/02/07/Bellman-iteration.png" alt=""></p> <p>其中有两种做法，一种是一次计算，同时更新，另一种是将更新后的值作为下一个格子的输入值进行更新。这两种做法都是可取的。</p> <p><strong>Policy iteration 策略迭代</strong></p> <p>具体做法是：先对所有状态初始化一个随机策略，然后使用第二种Bellman equation，进行最佳策略的更新，然后不断迭代，直到收敛。</p> <p><img src="http://uricc.ga/images/2020/02/07/Bellman-iteration2.png" alt=""></p> <h3 id="_4-monte-carlo-method-奖励函数的求解方法2"><a href="#_4-monte-carlo-method-奖励函数的求解方法2" class="header-anchor">#</a> 4. Monte-Carlo Method - 奖励函数的求解方法2</h3> <p><img src="http://uricc.ga/images/2020/02/07/_20200207231312f43e8b578cd59838.png" alt=""></p> <p>正常情况下一般来讲，各状态之间的Transition Probability是未知的。</p> <p>蒙特卡洛是用随机模拟的方式，来估算每一个状态的V值的方法。</p> <p><img src="http://latex.codecogs.com/gif.latex?S_0%5Cxrightarrow%5Br%5D%7Ba_0%7DS_1%5Cxrightarrow%5Br%5D%7Ba_1%7DS_2%5Cxrightarrow%5Br%5D%7Ba_2%7D%5Ccdots%5Cxrightarrow%5Br%5D%7Ba_%7Bt-1%7D%7DS_T;%5Ctextcircled%7B1%7D" alt=""></p> <p><img src="http://latex.codecogs.com/gif.latex?S_0%5E%7B'%7D%5Cxrightarrow%5Br%5D%7Ba_0%7DS_1%5E%7B'%7D%5Cxrightarrow%5Br%5D%7Ba_1%7DS_2%5E%7B'%7D%5Cxrightarrow%5Br%5D%7Ba_2%7D%5Ccdots%5Cxrightarrow%5Br%5D%7Ba_%7Bt-1%7D%7DS_T%5E%7B'%7D;%5Ctextcircled%7B2%7D" alt=""></p> <p>……</p> <p>通过不同的模拟，我们可以得到不同的路线，而每次模拟都可以算出路线上每个状态点的<img src="http://latex.codecogs.com/gif.latex?G_t" alt="">值。</p> <p>而真<img src="http://latex.codecogs.com/gif.latex?G_t" alt="">值的计算有2种方法：</p> <p><strong>① First Visited - 每个状态在每条路径只计算第一次</strong></p> <p><img src="http://latex.codecogs.com/gif.latex?V(S_1)=%5Cfrac%7BG_t%5E1+G_t%5E2+G_t%5E3+...+G_t%5En%7D%7Bn%7D" alt=""></p> <p><strong>② Every Visited - 每个状态碰见几次就是几次</strong></p> <p><img src="http://latex.codecogs.com/gif.latex?V(S_1)=%5Cfrac%7BG_t%5E1%7B(S_1)%7D+G_t%5E2%7B(S_1)%7D+G_t%5E3%7B(S_1)%7D+...+G_t%5En%7B(S_1)%7D%7D%7Bn%7D" alt=""></p> <p>总体来讲，蒙特卡洛方法bias低（为0），variance高（由于随机采样）</p> <p>DP法bias高（需要估计概率值），variance低（不需随机采样，直接计算）</p> <h3 id="_5-temporal-difference-learning-奖励函数的求解方法3"><a href="#_5-temporal-difference-learning-奖励函数的求解方法3" class="header-anchor">#</a> 5. Temporal-Difference Learning - 奖励函数的求解方法3</h3> <ul><li>Temporal-Difference Learning是DP与Monte-Carlo的结合。</li></ul> <p>即，一部分用蒙特卡洛采样解，一部分用Bellman Equation解。</p> <p><img src="http://latex.codecogs.com/gif.latex?%5Ctextcircled%7B,%7D%5Clongrightarrow%5Ctextcircled%7B,%7D%5Clongrightarrow%5Ctextcircled%7B,%7D%5Clongrightarrow%7BV(S_%7Bt+2%7D)%7D" alt=""></p> <p>例如，前面几步用蒙特卡洛，后面几步用DP。（圆圈表示采样）</p> <p>TD相较于蒙特卡洛，bias升高（需要用概率估计值），variance降低（减少了随机采样）</p> <p>TD相较于蒙特卡洛，计算更快；相较于DP，不需要知道Transition Probability。</p> <h3 id="_6-q-learning-and-sarsa"><a href="#_6-q-learning-and-sarsa" class="header-anchor">#</a> 6. Q-learning and SARSA</h3> <p>Q-learning: 计算出不同的Q后，Value取值按照max()取</p> <p>SARSA (State-Action-Reward-State-Action): 计算出不同的Q后，Value取值按照policy取</p> <h3 id="_7-function-approximation-deep-q-network"><a href="#_7-function-approximation-deep-q-network" class="header-anchor">#</a> 7. Function Approximation (Deep Q-Network)</h3> <p>Deep Q-Network的思路是使用神经网络来拟合Q函数：f(S1) → Q值</p> <p>该神经网络的输入就是State，输出就是Q-Value</p> <p><img src="http://uricc.ga/images/2020/02/07/Annotation-2020-02-08-020129.png" alt=""></p> <p>（例如，Atari游戏中，输入的就是图片像素，使用CNN提取特征）</p> <h3 id="_8-policy-gradient"><a href="#_8-policy-gradient" class="header-anchor">#</a> 8. Policy Gradient</h3> <p>给一个函数，这个函数可以拟合出最佳move，即π的学习。</p> <p><img src="http://latex.codecogs.com/gif.latex?maximize;J(%5Ctheta)=%5Csum_%7Ba%5Cin%7BA%7D%7D%7B%5Cpi(a%7Cs,%5Ctheta)%7DQ_%5Cpi(s,a)" alt=""></p> <p><strong>AlphaGo</strong></p> <p>先通过人类棋谱学习，再通过自己对弈学习。</p> <p><strong>AlphaZero</strong></p> <p>棋盘输入13层的CNN提取状态特征，并且加上一些额外信息（例如棋盘中可被吃掉的目，等）。</p> <p>通过自己和自己下棋，更新policy gradient中的参数。其中使用了蒙特卡洛树搜索，剪枝等方法。</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/notebook/assets/js/app.ecc66517.js" defer></script><script src="/notebook/assets/js/2.4b33a2ac.js" defer></script><script src="/notebook/assets/js/38.cc1cb47e.js" defer></script>
  </body>
</html>
