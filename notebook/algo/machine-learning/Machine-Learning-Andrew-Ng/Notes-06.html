<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Simon’s Notebook</title>
    <meta name="generator" content="VuePress 1.5.3">
    <link rel="icon" href="/notebook/doraemon_winter_circle.png">
    <meta name="description" content="A Technical Notebook - Simon的技术笔记">
    <link rel="preload" href="/notebook/assets/css/0.styles.8d6e7e33.css" as="style"><link rel="preload" href="/notebook/assets/js/app.8d20faf4.js" as="script"><link rel="preload" href="/notebook/assets/js/2.4f4438fc.js" as="script"><link rel="preload" href="/notebook/assets/js/4.415236f2.js" as="script"><link rel="prefetch" href="/notebook/assets/js/10.e27e6bfd.js"><link rel="prefetch" href="/notebook/assets/js/11.a9bdaef7.js"><link rel="prefetch" href="/notebook/assets/js/12.362e1281.js"><link rel="prefetch" href="/notebook/assets/js/13.79414429.js"><link rel="prefetch" href="/notebook/assets/js/14.8edd2b1c.js"><link rel="prefetch" href="/notebook/assets/js/15.4d95b94a.js"><link rel="prefetch" href="/notebook/assets/js/16.80e14f74.js"><link rel="prefetch" href="/notebook/assets/js/17.5d2d5722.js"><link rel="prefetch" href="/notebook/assets/js/18.90758c3d.js"><link rel="prefetch" href="/notebook/assets/js/19.aaa1abbd.js"><link rel="prefetch" href="/notebook/assets/js/20.50eb8285.js"><link rel="prefetch" href="/notebook/assets/js/21.17c1a6a6.js"><link rel="prefetch" href="/notebook/assets/js/22.60a4a42c.js"><link rel="prefetch" href="/notebook/assets/js/23.0917c603.js"><link rel="prefetch" href="/notebook/assets/js/24.f55f020a.js"><link rel="prefetch" href="/notebook/assets/js/25.39ae2e9c.js"><link rel="prefetch" href="/notebook/assets/js/26.133e3145.js"><link rel="prefetch" href="/notebook/assets/js/27.ede3f16e.js"><link rel="prefetch" href="/notebook/assets/js/28.fb747359.js"><link rel="prefetch" href="/notebook/assets/js/29.47671f43.js"><link rel="prefetch" href="/notebook/assets/js/3.28ee897f.js"><link rel="prefetch" href="/notebook/assets/js/30.5941d8f6.js"><link rel="prefetch" href="/notebook/assets/js/31.57e61126.js"><link rel="prefetch" href="/notebook/assets/js/32.41de60fc.js"><link rel="prefetch" href="/notebook/assets/js/33.0dfd42c0.js"><link rel="prefetch" href="/notebook/assets/js/34.bdab3f8e.js"><link rel="prefetch" href="/notebook/assets/js/35.4d6488e5.js"><link rel="prefetch" href="/notebook/assets/js/36.79031677.js"><link rel="prefetch" href="/notebook/assets/js/37.169fd0c3.js"><link rel="prefetch" href="/notebook/assets/js/38.47f89690.js"><link rel="prefetch" href="/notebook/assets/js/39.d5cd159c.js"><link rel="prefetch" href="/notebook/assets/js/40.077940df.js"><link rel="prefetch" href="/notebook/assets/js/41.01e5a580.js"><link rel="prefetch" href="/notebook/assets/js/42.e8e6a19d.js"><link rel="prefetch" href="/notebook/assets/js/43.53f5a11c.js"><link rel="prefetch" href="/notebook/assets/js/44.7702b0cf.js"><link rel="prefetch" href="/notebook/assets/js/5.903e51f7.js"><link rel="prefetch" href="/notebook/assets/js/6.e7943522.js"><link rel="prefetch" href="/notebook/assets/js/7.56edb5f9.js"><link rel="prefetch" href="/notebook/assets/js/8.dbed8ab7.js"><link rel="prefetch" href="/notebook/assets/js/9.8873aef5.js">
    <link rel="stylesheet" href="/notebook/assets/css/0.styles.8d6e7e33.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/notebook/" class="home-link router-link-active"><img src="/notebook/doraemon_winter_circle.png" alt="Simon’s Notebook" class="logo"> <span class="site-name can-hide">Simon’s Notebook</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/notebook/guide/" class="nav-link">
  导航
</a></div><div class="nav-item"><a href="/notebook/resource/" class="nav-link">
  资源
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><span class="title">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/algo/" class="nav-link router-link-active">
  算法导航页
</a></li><li class="dropdown-item"><h4>
          算法与数据结构
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/algo/ds-algo/pending.html" class="nav-link">
  算法与数据结构
</a></li><li class="dropdown-subitem"><a href="/notebook/algo/ds-algo/leetcode/" class="nav-link">
  Leetcode刷题总结
</a></li></ul></li><li class="dropdown-item"><h4>
          机器学习
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/algo/machine-learning/ml_big_map.html" class="nav-link">
  机器学习算法
</a></li><li class="dropdown-subitem"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-01.html" class="nav-link">
  吴恩达《机器学习公开课》笔记
</a></li></ul></li></ul></div></div><div class="nav-item"><a href="/notebook/nlp/" class="nav-link">
  NLP
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="/cs-basics/" class="dropdown-title"><span class="title">计算机基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/cs-basics/network/" class="nav-link">
  计算机网络
</a></li></ul></div></div><div class="nav-item"><a href="/notebook/reading/" class="nav-link">
  阅读
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/notebook/guide/" class="nav-link">
  导航
</a></div><div class="nav-item"><a href="/notebook/resource/" class="nav-link">
  资源
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><span class="title">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/algo/" class="nav-link router-link-active">
  算法导航页
</a></li><li class="dropdown-item"><h4>
          算法与数据结构
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/algo/ds-algo/pending.html" class="nav-link">
  算法与数据结构
</a></li><li class="dropdown-subitem"><a href="/notebook/algo/ds-algo/leetcode/" class="nav-link">
  Leetcode刷题总结
</a></li></ul></li><li class="dropdown-item"><h4>
          机器学习
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/notebook/algo/machine-learning/ml_big_map.html" class="nav-link">
  机器学习算法
</a></li><li class="dropdown-subitem"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-01.html" class="nav-link">
  吴恩达《机器学习公开课》笔记
</a></li></ul></li></ul></div></div><div class="nav-item"><a href="/notebook/nlp/" class="nav-link">
  NLP
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="/cs-basics/" class="dropdown-title"><span class="title">计算机基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/notebook/cs-basics/network/" class="nav-link">
  计算机网络
</a></li></ul></div></div><div class="nav-item"><a href="/notebook/reading/" class="nav-link">
  阅读
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>机器学习</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/notebook/algo/machine-learning/ml_big_map.html" class="sidebar-link">机器学习概览</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>吴恩达《机器学习》笔记</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-01.html" class="sidebar-link">1.机器学习简介</a></li><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-02.html" class="sidebar-link">2.一元线性回归</a></li><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-03.html" class="sidebar-link">3.线性代数复习（Optional）</a></li><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-04.html" class="sidebar-link">4.多元线性回归</a></li><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-05.html" class="sidebar-link">5.Octave教程</a></li><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-06.html" aria-current="page" class="active sidebar-link">6.逻辑回归</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-06.html#_6-1-分类" class="sidebar-link">6.1 分类</a></li><li class="sidebar-sub-header"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-06.html#_6-2-模型表示" class="sidebar-link">6.2 模型表示</a></li><li class="sidebar-sub-header"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-06.html#_6-3-决策边界的线性与非线性" class="sidebar-link">6.3 决策边界的线性与非线性</a></li><li class="sidebar-sub-header"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-06.html#_6-4-逻辑回归损失函数" class="sidebar-link">6.4 逻辑回归损失函数</a></li><li class="sidebar-sub-header"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-06.html#_6-5-简化损失函数与梯度下降" class="sidebar-link">6.5 简化损失函数与梯度下降</a></li><li class="sidebar-sub-header"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-06.html#_6-6-高级优化算法" class="sidebar-link">6.6 高级优化算法</a></li><li class="sidebar-sub-header"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-06.html#_6-7-多分类：一对多" class="sidebar-link">6.7 多分类：一对多</a></li></ul></li><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-07.html" class="sidebar-link">7.正则化</a></li><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-08.html" class="sidebar-link">8.神经网络: 表征</a></li><li><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-09.html" class="sidebar-link">9.神经网络: 学习</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><p>本内容按照吴恩达公开课《Machine Learning》的 Lecture Slides 进行分类，每一个H1标题对应一个Lecture Slide，每一个H2标题对应Lecture Slide中的一个小章节。</p> <p>本内容是课程的简化总结，适合已经了解机器学习基本概念的人进行回顾以及查漏补缺。</p> <h1 id="_6-逻辑回归"><a href="#_6-逻辑回归" class="header-anchor">#</a> 6 逻辑回归</h1> <h2 id="_6-1-分类"><a href="#_6-1-分类" class="header-anchor">#</a> 6.1 分类</h2> <p>逻辑回归用于<strong>分类问题</strong>的场景，例如：</p> <ul><li>邮件是否是垃圾邮件？</li> <li>订单是否是欺诈订单？</li> <li>肿瘤是良性还是恶性？</li></ul> <p>此时 $y\in{0,1}$，一般把0当作负例，1当作正例</p> <p><strong>阈值的选择</strong>：可以自定义，例如：</p> <p>当 $h_{\theta}(x)\geq0.5$ 时，预测 $y=1$</p> <p>当 $h_{\theta}(x)\lt0.5$ 时，预测 $y=0$</p> <blockquote><p>编者注1：后来我们知道阈值的选择会对应不同的模型表现，通过衡量模型在<strong>不同阈值下的总体表现</strong>，就产生了<strong>ROC曲线</strong>，通过计算ROC曲线下的面积，就产生了<strong>AUC评价指标</strong>。</p> <p>编者注2：**为什么要使用逻辑回归呢？**通过线性函数拟合，然后选择恰当的阈值来得到结果不可以吗？可以，但是这样做有两个问题：一、阈值的选择会因此变得非常敏感；二、线性函数会因此无法很好地拟合数据。其实本质上来讲，使用sigmoid函数将连续空间转化到[0, 1]空间，就是为了更好地拟合数据分布的空间。</p> <p>编者注3：进一步思考，如果采用线性模型，仅在损失的计算中应用sigmoid（例如logit loss），那么模型的拟合与逻辑回归其实是一样了，此时也可以通过选择一个恰当的y的阈值来定义模型。此时可以理解为把逻辑回归的sigmod层脱离了出来，这里的y就是逻辑回归中的隐藏层y（加sigmoid函数之前）</p></blockquote> <h2 id="_6-2-模型表示"><a href="#_6-2-模型表示" class="header-anchor">#</a> 6.2 模型表示</h2> <p>在线性回归的基础上：增加一个函数 $g$，使得：</p> <p>$h_{\theta}(x)=g(\theta^Tx)$</p> <p>如果 $g$ 使用Sigmoid函数(也叫Logistic函数)，则模型就叫逻辑回归(Logistic Regression)</p> <p>$g(z)=\frac{1}{1+e^{-z}}$</p> <p>于是，整个模型就是 $h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}$</p> <p>由于逻辑回归的输出为(0, 1)空间，所以可以解释为概率值，例：如果 $h_\theta{(x)}=0.7$ 则一个病人的肿瘤为恶性的概率为70%。</p> <h2 id="_6-3-决策边界的线性与非线性"><a href="#_6-3-决策边界的线性与非线性" class="header-anchor">#</a> 6.3 决策边界的线性与非线性</h2> <p>当输入g(z)的<strong>模型函数为线性函数</strong>时，<strong>决策边界就是线性</strong>的。例如，输入线性函数情况下定义决策边界为0.5，则当 $h_\theta{(x)}\geq0.5$ 时，$\theta^Tx$ 就 $\geq0$，此时 $\theta^Tx$ 就是二维平面内划分正负样例的一条直线：</p> <p><img src="/notebook/assets/img/image-20200809200733364.3dde3917.png" alt="image-20200809201240355"></p> <p>当输入g(z)的<strong>函数为非线性函数</strong>时，<strong>决策边界就可以是非线性</strong>的。例如，当输入函数为$z{(x)}=\theta_0+\theta_1{x_1}+\theta_2{x_2}+\theta_3{x_1^2}+\theta_4{x_2^2}$ 时，函数为抛物面，</p> <p><img src="https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=2532408450,1088232931&amp;fm=26&amp;gp=0.jpg" alt="img"></p> <p>此时的<strong>决策边界</strong>就可以是一个<strong>圆/椭圆</strong>：</p> <p><img src="/notebook/assets/img/image-20200809201050891.48372e4f.png" alt="image-20200809201847083"></p> <h2 id="_6-4-逻辑回归损失函数"><a href="#_6-4-逻辑回归损失函数" class="header-anchor">#</a> 6.4 逻辑回归损失函数</h2> <p>逻辑回归的损失函数如下：</p> <p>$Cost(h_\theta{(x)},y)=\begin{cases}-log(h_\theta(x))&amp;if;y=1\-log(1-h_\theta(x))&amp;if;y=0\end{cases}$</p> <p>其图像如下：</p> <p><img src="/notebook/assets/img/image-20200809204653140.18433ca3.png" alt="image-20200809204653140"></p> <h2 id="_6-5-简化损失函数与梯度下降"><a href="#_6-5-简化损失函数与梯度下降" class="header-anchor">#</a> 6.5 简化损失函数与梯度下降</h2> <p><strong>简化后的损失函数</strong>为：</p> <p>$J(\theta)=-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}\log{h_\theta{(x^{(i)})}}+(1-y^{(i)})\log{(1-h_\theta{(x^{(i)})})}]$</p> <p>同时考虑了y=1和y=0的情况，并且把负号提到了最前面。</p> <blockquote><p>编者注：这个损失函数就叫做<strong>Log损失</strong>或者<strong>Logistic损失</strong>或者<strong>Logit损失</strong>或者<strong>Logarithmic损失</strong>，都是同一个东西。参考: <a href="https://stats.stackexchange.com/a/224135/291675" target="_blank" rel="noopener noreferrer">Reference<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></blockquote> <p><strong>逻辑回归的梯度下降</strong>：</p> <p>对$J(\theta)$求导，可得：$\frac{\part}{\part\theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}({x^{(i)})}-y^{(i)})\cdot{x_{j}^{(i)}}$</p> <p>因此如果要最小化 $J(\theta)$，算法为：</p> <p>重复 $\theta_j:=\theta_j-\alpha\sum_{i=1}^{m}(h_{\theta}({x^{(i)})}-y^{(i)})\cdot{x_{j}^{(i)}}$ 直到<strong>函数收敛</strong>。</p> <p>可以发现<strong>这个表达式和最小二乘法的线性回归几乎一样</strong>！</p> <h2 id="_6-6-高级优化算法"><a href="#_6-6-高级优化算法" class="header-anchor">#</a> 6.6 高级优化算法</h2> <p>除了梯度下降，还有一些更高级的优化算法，例如：</p> <ul><li>共轭梯度法 (<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method" target="_blank" rel="noopener noreferrer">Conjugate Gradient<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>)</li> <li><a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" target="_blank" rel="noopener noreferrer">BFGS<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>，属于拟牛顿法的一种，以四位数学家Broyden, Fletcher, Goldfarb, Shanno的首字母命名</li> <li><a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" target="_blank" rel="noopener noreferrer">L-BFGS<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>，即Limited BFGS，是在有限内存中求BFGS的一种方法</li></ul> <p>这些方法<strong>无需设定学习率</strong>，一般<strong>比梯度下降更快</strong>，但缺点是<strong>计算复杂度更高</strong></p> <p>在Octave中可以用fminunc函数来使用这些优化算法：</p> <div class="language-octave extra-class"><pre class="language-text"><code>options = optimset('GradObj', 'on', 'MaxIter', 100);
initialTheta = zeros(2,1);
   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
</code></pre></div><h2 id="_6-7-多分类：一对多"><a href="#_6-7-多分类：一对多" class="header-anchor">#</a> 6.7 多分类：一对多</h2> <p>对于多分类问题（大于等于3个类别），可以采用一对多方式，对每个类别都训练一个二分类模型（把当前类为1当作正类），然后在预测时把样本输入到每个模型中，最后选择的类别为最大的输出概率所对应的类别：$\underset{i}{\operatorname{argmax}}h_\theta^{(i)}(x)$</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-05.html" class="prev">
        5.Octave教程
      </a></span> <span class="next"><a href="/notebook/algo/machine-learning/Machine-Learning-Andrew-Ng/Notes-07.html">
        7.正则化
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/notebook/assets/js/app.8d20faf4.js" defer></script><script src="/notebook/assets/js/2.4f4438fc.js" defer></script><script src="/notebook/assets/js/4.415236f2.js" defer></script>
  </body>
</html>
