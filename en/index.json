[{"categories":["Machine Learning"],"content":"This article visualizes L1 \u0026 L2 Regularization, with Cross Entropy Loss as the base loss function. Moreover, the Visualization shows how L1 \u0026 L2 Regularization could affect the original surface of cross entropy loss. Although the concept is not difficult, the visualization do make understanding of L1 \u0026 L2 regularization easier. For example why L1-reg often leads to sparse model. Above all, the visualization itself is indeedly beautiful. ","date":"2021-04-15","objectID":"/en/posts/2021/cross-entropy-loss-visualized/:0:0","tags":["Machine Learning","Deep Learning","Visualization","Regularization"],"title":"What does L1 \u0026 L2 Regularization Look Like?","uri":"/en/posts/2021/cross-entropy-loss-visualized/"},{"categories":["Machine Learning"],"content":"1. Cross Entropy Loss ¬∂Consider a super simple neural network: a super simple neural networksimple_nn \"\ra super simple neural network\r The forward propogation process of the network would be: $$\\hat{z_1}=\\beta_1x$$ $$\\hat{z_2}=\\beta_2x$$ $$Softmax(\\hat{z_i}),\\ i\\in{2}$$ Consider a cross entropy loss: $$J(\\beta)=-p\\log(q)-(1-p)\\log(1-q)$$ $$=-p\\log(\\frac{e^{\\beta_1x}}{e^{\\beta_1x}+e^{\\beta_2x}})-(1-p)\\log(\\frac{e^{\\beta_2x}}{e^{\\beta_1x}+e^{\\beta_2x}})$$ $$=‚Ä¶$$ $$=-p\\log{e^{\\beta_1x}}-(1-p)log{e^{\\beta_2x}}+log(e^{\\beta_1x}+e^{\\beta_2x})$$ $$=-p\\beta_1x-(1-p)\\beta_2x+log(e^{\\beta_1x}+e^{\\beta_2x})$$ where $p$ denotes the true label of $z_1$ and $1-p$ denotes the true label of $z_2$, $\\beta_1$ and $\\beta_2$ are model parameters, $x$ denotes the model input and is a scalar. So then, the cross entropy loss could be visualized as: \r\r\rvar myChart = echarts.init(document.getElementById('echarts400'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0;\\r\\n l2a = 0;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r (For convenience, p is set to 1 and x is set to 1. Because we only want to see how loss varies to different parameter sets $\\beta_1$ and $\\beta_2$) We could see a smooth curved surface down towards the ground. ","date":"2021-04-15","objectID":"/en/posts/2021/cross-entropy-loss-visualized/:1:0","tags":["Machine Learning","Deep Learning","Visualization","Regularization"],"title":"What does L1 \u0026 L2 Regularization Look Like?","uri":"/en/posts/2021/cross-entropy-loss-visualized/"},{"categories":["Machine Learning"],"content":"2. Cross Entropy Loss with L1 Regularization ¬∂Generally, to prevent parameters endlessly fitting to a great number, and to solve overfitting, we could apply regularization. L1-Regularization regularizes weights by adding the sum of L1-norm of all parameters to loss function: $$J(\\beta)=-p\\beta_1x-(1-p)\\beta_2x+log(e^{\\beta_1x}+e^{\\beta_2x})+\\lambda{(||\\beta_1||_1+||\\beta_2||_1)}$$ The following graphs show different surfaces upon different L1-reg weights: \r\r\rvar myChart = echarts.init(document.getElementById('echarts401'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L1 Reg\\\\n(Œª=0.2)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0.2;\\r\\n l2a = 0;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r \r\r\rvar myChart = echarts.init(document.getElementById('echarts402'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L1 Reg\\\\n(Œª=0.4)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0.4;\\r\\n l2a = 0;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r \r\r\rvar myChart = echarts.init(document.getElementById('echarts403'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L1 Reg\\\\n(Œª=0.6)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },","date":"2021-04-15","objectID":"/en/posts/2021/cross-entropy-loss-visualized/:2:0","tags":["Machine Learning","Deep Learning","Visualization","Regularization"],"title":"What does L1 \u0026 L2 Regularization Look Like?","uri":"/en/posts/2021/cross-entropy-loss-visualized/"},{"categories":["Machine Learning"],"content":"3. Cross Entropy Loss with L2 Regularization ¬∂Another common one is to apply L2-Regularization, by adding the sum of L2-norm of all parameters: $$J(\\beta)=-p\\beta_1x-(1-p)\\beta_2x+log(e^{\\beta_1x}+e^{\\beta_2x})+\\Omega{(||\\beta_1||_2^2+||\\beta_2||_2^2)}$$ The following graphs show different surfaces upon different L2-reg weights: \r\r\rvar myChart = echarts.init(document.getElementById('echarts399'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L2 Reg\\\\n(Œ©=0.1)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0;\\r\\n l2a = 0.1;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r \r\r\rvar myChart = echarts.init(document.getElementById('echarts398'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L2 Reg\\\\n(Œ©=0.2)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0;\\r\\n l2a = 0.2;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r \r\r\rvar myChart = echarts.init(document.getElementById('echarts397'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L2 Reg\\\\n(Œ©=0.3)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dim","date":"2021-04-15","objectID":"/en/posts/2021/cross-entropy-loss-visualized/:3:0","tags":["Machine Learning","Deep Learning","Visualization","Regularization"],"title":"What does L1 \u0026 L2 Regularization Look Like?","uri":"/en/posts/2021/cross-entropy-loss-visualized/"},{"categories":["Machine Learning"],"content":"4. Cross Entropy Loss with L1+L2 Regularization ¬∂L1 and L2 Regularization can take place at the same time, which is like: \r\r\rvar myChart = echarts.init(document.getElementById('echarts395'));\rvar option = eval('('+\"\\r\\n{\\r\\n \\\"title\\\": {\\r\\n \\\"text\\\": \\\"Cross Entropy Loss Visualized with L1+L2 Reg\\\\n(Œª=0.1ÔºåŒ©=0.4)\\\",\\r\\n \\\"left\\\": \\\"center\\\"\\r\\n },\\r\\n \\\"tooltip\\\": {\\\"trigger\\\": \\\"axis\\\"},\\r\\n \\\"backgroundColor\\\": \\\"#fff\\\",\\r\\n \\\"visualMap\\\": {\\r\\n \\\"show\\\": false,\\r\\n \\\"dimension\\\": 2,\\r\\n \\\"min\\\": 0.5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"inRange\\\": {\\r\\n \\\"color\\\": [\\\"#313695\\\", \\\"#4575b4\\\", \\\"#74add1\\\", \\\"#abd9e9\\\", \\\"#e0f3f8\\\", \\\"#ffffbf\\\", \\\"#fee090\\\", \\\"#fdae61\\\", \\\"#f46d43\\\", \\\"#d73027\\\", \\\"#a50026\\\"]\\r\\n }\\r\\n },\\r\\n \\\"xAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤1\\\"\\r\\n },\\r\\n \\\"yAxis3D\\\": {\\r\\n \\\"max\\\": 5,\\r\\n \\\"min\\\": -5,\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Œ≤2\\\"\\r\\n },\\r\\n \\\"zAxis3D\\\": {\\r\\n \\\"type\\\": \\\"value\\\",\\r\\n \\\"name\\\": \\\"Loss\\\"\\r\\n },\\r\\n \\\"grid3D\\\": {\\r\\n \\\"viewControl\\\": {\\r\\n // \\\"projection\\\": \\\"orthographic\\\"\\r\\n }\\r\\n },\\r\\n \\\"series\\\": [{\\r\\n \\\"type\\\": \\\"surface\\\",\\r\\n \\\"wireframe\\\": {\\r\\n // \\\"show\\\": false\\r\\n },\\r\\n \\\"equation\\\": {\\r\\n \\\"x\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5, \\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"y\\\": {\\r\\n \\\"min\\\": -5,\\r\\n \\\"max\\\": 5,\\r\\n \\\"step\\\": 0.1\\r\\n },\\r\\n \\\"z\\\": function (b1, b2) {\\r\\n ptrue = 1;\\r\\n x = 1;\\r\\n l1a = 0.4;\\r\\n l2a = 0.1;\\r\\n \\u003c!-- return (ptrue*Math.exp(b2*x)-(1-ptrue)*Math.exp(b1*x))/(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2); --\\u003e\\r\\n return -ptrue*b1*x-(1-ptrue)*b2*x+Math.log(Math.exp(b1*x)+Math.exp(b2*x))+l1a*(Math.abs(b1)+Math.abs(b2))+l2a*(b1**2+b2**2);\\r\\n }\\r\\n }\\r\\n }]\\r\\n}\\r\\n\"+')');\rmyChart.setOption(option);\r ","date":"2021-04-15","objectID":"/en/posts/2021/cross-entropy-loss-visualized/:4:0","tags":["Machine Learning","Deep Learning","Visualization","Regularization"],"title":"What does L1 \u0026 L2 Regularization Look Like?","uri":"/en/posts/2021/cross-entropy-loss-visualized/"},{"categories":["Machine Learning"],"content":"5. Conslusion ¬∂L1 Regularization Penalizes sum of absolute value of weights, which results in a sparse model Sparse model is cater to feature selection Sparse model is simple and interpretable, but cannot learn complex patterns Robust to outliers L2 Regularization Penalizes sum of squared value of weights, which results in a dense model Learns complex patterns and generally gives better prediction Sensitive to outliers ","date":"2021-04-15","objectID":"/en/posts/2021/cross-entropy-loss-visualized/:5:0","tags":["Machine Learning","Deep Learning","Visualization","Regularization"],"title":"What does L1 \u0026 L2 Regularization Look Like?","uri":"/en/posts/2021/cross-entropy-loss-visualized/"},{"categories":["Life Moments"],"content":"I figured out how to extract the PPD file from Lenovo printer driver and connect it to my Raspberry Pi. ","date":"2020-07-25","objectID":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/","tags":["Raspberry Pi","Printer"],"title":"Adding Lenovo Printer to Raspberry Pi (or Other Linux Computers)","uri":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["Life Moments"],"content":"One of the main benefits of connecting the printer to Raspberry Pi is to enable remote printing service, even if the printer is a cabled one. There are already a lot of posts on the internet discussing how you can set up your Raspberry Pi and connect it with your printer (with CUPS), like How to add a printer to your raspberry pi or other Linux Computer. So this is not the main focus of this post. The focus of this post is on how to extract a PPD file from the original printer driver package provided by the manufacturer, which is needed by the Linux system (and of course, Raspberry Pi), when CUPS doesn‚Äôt have native support on it. Specifically, I am using my Lenovo M7605D as an example. ","date":"2020-07-25","objectID":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/:0:0","tags":["Raspberry Pi","Printer"],"title":"Adding Lenovo Printer to Raspberry Pi (or Other Linux Computers)","uri":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["Life Moments"],"content":"PostScript Printer Description (PPD) File ¬∂Developed by Adobe, the PPD file (Wikipedia) is a piece of information that describes the entire set of features and capabilities available for PostScript printer. Basically it describes how the printer should organize and print the contents when documents are sent to the system printing service. ","date":"2020-07-25","objectID":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/:1:0","tags":["Raspberry Pi","Printer"],"title":"Adding Lenovo Printer to Raspberry Pi (or Other Linux Computers)","uri":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["Life Moments"],"content":"Locating the PPD file ¬∂Once downloaded the driver package (which is an ISO file) from Lenovo Driver Page, extract it and open the folder /install, you‚Äôll see a lot of model folders. The Driver Pageimage-20200725212618877 \"\rThe Driver Page\r The ‚Äò/install‚Äô folder after extraction\"\rThe ‚Äò/install‚Äô folder after extraction\r Going into that you‚Äôll see a file /install/M7605D/chneng/Brinst_Lang.ini which specifies the folder for the Post-Script driver for the model: The location of PostScript driver\"\rThe location of PostScript driver\r Going into that dir you‚Äôll see some .pp_ files, and that‚Äôs what we want Driver found! It seems my model m7605d is using the same driver as m7675dxf/m7615dna\"\rDriver found! It seems my model m7605d is using the same driver as m7675dxf/m7615dna\r ","date":"2020-07-25","objectID":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/:2:0","tags":["Raspberry Pi","Printer"],"title":"Adding Lenovo Printer to Raspberry Pi (or Other Linux Computers)","uri":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["Life Moments"],"content":"Uncompressing the .pp_ file ¬∂It turns out that the ppd file provided by Lenovo, which has ‚Äòpp_‚Äô as suffix, is compressed. (originally I thought it is encrypted when I saw those gibberish in the file but it is actually not) The file head ‚ÄòSZDD‚Äô in the beginning shows that it is an old compressed format, which is rare to see now\"\rThe file head ‚ÄòSZDD‚Äô in the beginning shows that it is an old compressed format, which is rare to see now\r SZDD file info on Wikipedia\"\rSZDD file info on Wikipedia\r Having a quick search on Google makes me aware of that this format could be uncompressed by MS-DOS EXPAND.EXE program. And surprisingly also, it could be directly uncompressed by 7-ZIP. It turns out that the file could be directly uncompressed by 7-ZIP, or in a more geek‚Äôs way, using windows ‚ÄòEXPAND‚Äô command\"\rIt turns out that the file could be directly uncompressed by 7-ZIP, or in a more geek‚Äôs way, using windows ‚ÄòEXPAND‚Äô command\r In the end, the .ppd file is nicely extracted. Done!\"\rDone!\r","date":"2020-07-25","objectID":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/:3:0","tags":["Raspberry Pi","Printer"],"title":"Adding Lenovo Printer to Raspberry Pi (or Other Linux Computers)","uri":"/en/posts/2020/adding-lenovo-printer-to-raspberry-pi/"},{"categories":["Life Moments"],"content":"Recently, I came up with a brilliant idea to remove the noise from the HDD in my old PC. ","date":"2020-07-24","objectID":"/en/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/","tags":["Life","Computer","Noise"],"title":"How to Noise-Free your HDD from an Old Computer?","uri":"/en/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/"},{"categories":["Life Moments"],"content":"Apart from fans, the HDD could be another main source of unpleasant noise in a old PC. Normally if your computer case is advanced enough, it would prepare you anti-vibration nail rubbers to remove the noise from HDD. However as an old computer which don‚Äôt have the condition, what can we do? ","date":"2020-07-24","objectID":"/en/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/:0:0","tags":["Life","Computer","Noise"],"title":"How to Noise-Free your HDD from an Old Computer?","uri":"/en/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/"},{"categories":["Life Moments"],"content":"A Basic Idea ¬∂Before coming up with the new idea, I firstly thought of removing the conduction of vibration between HDD and the case with rubber bands, which the result turns out to be quite good. Having nearly the whole HDD plastered with rubber bands makes it not only less noise conducted but also safer (in case if I wanna move the computer here and there sometime). Old method, which has already removed most of the noise\"\rOld method, which has already removed most of the noise\r After applied, the noise is largely decreased and I wouldn‚Äôt notice anymore if not paying attention carefully. ","date":"2020-07-24","objectID":"/en/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/:1:0","tags":["Life","Computer","Noise"],"title":"How to Noise-Free your HDD from an Old Computer?","uri":"/en/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/"},{"categories":["Life Moments"],"content":"A Better Idea, Even More Effective ¬∂Recently, I suddenly came up with a brilliant idea which totally removes the contact between HDD and the case, and it truly did. That is to make a rubber rope bridge between the gap and put the HDD on it. New method, which turns out to be even more effective\"\rNew method, which turns out to be even more effective\r And it turns out to be a critical hit! Since the vibration is totally absorbed by the rubber rope, the noise has totally disappeared. Finally, I‚Äôve totally resolved the noise issue of the 8-year-old HDDÔºÅ ","date":"2020-07-24","objectID":"/en/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/:2:0","tags":["Life","Computer","Noise"],"title":"How to Noise-Free your HDD from an Old Computer?","uri":"/en/posts/2020/how-to-noise-free-your-hdd-from-an-old-computer/"},{"categories":null,"content":"‚Äã Hello, and what a great honor to have you here! I am Simon, a tech lover, hopefully my blog could be of some help to you! ","date":"2020-06-28","objectID":"/en/about/:0:0","tags":null,"title":"About Me","uri":"/en/about/"},{"categories":null,"content":"Who am I ‚Ä¶ ¬∂ A dreamer An NLPer An enthusiast of computer, music, games A proponent and practitioner of Feynman Technique A boy who wants to understand everything in deep ","date":"2020-06-28","objectID":"/en/about/:0:1","tags":null,"title":"About Me","uri":"/en/about/"},{"categories":null,"content":"The Purpose of This Blog ¬∂ ‚úîÔ∏è To share some of my opinions of the world ‚úîÔ∏è To record my life moments ‚úîÔ∏è ‚ÄãTo keep myself a peace land in the era of exploding information (or probably, contributing to itüôÉ) \r2. **Sentiment Analysis SystemÔºö**\rThe system can retrieve information from a piece of comment with 20 dimensions, the data of the model comes from AI Challenger 2018. Demo: Sentiment System [Alternative¬†Link]\r --\r","date":"2020-06-28","objectID":"/en/about/:0:2","tags":null,"title":"About Me","uri":"/en/about/"},{"categories":null,"content":"Values ¬∂Names Don‚Äôt Constitute Knowledge - Rechard Feymann Learn by playing - the Super Mario Effect ","date":"2020-06-28","objectID":"/en/about/:0:3","tags":null,"title":"About Me","uri":"/en/about/"},{"categories":null,"content":"Contact ¬∂Mail: yihua.zhou@outlook.com WeChat: astro1boy Github: @SimZhou Twitter: @zhouyh0102 ","date":"2020-06-28","objectID":"/en/about/:0:4","tags":null,"title":"About Me","uri":"/en/about/"},{"categories":[],"content":"Constructing‚Ä¶ ","date":"0001-01-01","objectID":"/en/gallery/:0:0","tags":[],"title":"My Gallery","uri":"/en/gallery/"}]